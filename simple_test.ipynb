{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Routing_Env(gym.Env):\n",
    "    \"\"\"\n",
    "    Description: Keeps track of each car's Own Env. \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.num_actions = config['num_paths']\n",
    "        self.soc_fac = config['soc_fac']\n",
    "        self.num_obs = 2\n",
    "        # Adding input for the comm channel\n",
    "        if config['comm']:\n",
    "            self.num_obs+=1\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Create initial observation for vehicle\n",
    "        prev_path = -1\n",
    "        prev_time = -1\n",
    "        self.state = [prev_path, prev_time]\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step_after_action_applied(self, path_choice, path_cost=0, marginal_cost=0):\n",
    "        \"\"\"\n",
    "        Note: Before you calculate the travel time, you need to update your network of the car's action choices first.\n",
    "              Otherwise, you won't be able to calculate the accurate travel times for each car. \n",
    "        \"\"\"\n",
    "        self.state = [path_choice, path_cost]\n",
    "        cost = (1-self.soc_fac)*path_cost + self.soc_fac*marginal_cost\n",
    "        rew = -cost\n",
    "        done = True  # We are always done after 1 step of the environment\n",
    "        return self.state, rew, done, {}\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return Box(\n",
    "            low = 0,\n",
    "            high = float('+inf'),\n",
    "            shape = (self.num_obs,),\n",
    "            dtype = np.float32\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return Discrete(self.num_actions)\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentRouting(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Cars all start at the same origin point, Point A, and need to reach the same destination, Point Z. \n",
    "        Each car can reach Point Z via a variety of routing choices described in a given network, in which \n",
    "        each route introduces different travel times and congestion. \n",
    "        The goal is to minimize the average travel times amongst each of the cars.\n",
    "    \n",
    "    Observation: \n",
    "        Type: Box(2) - for partially observed env\n",
    "        Num\tObservation                 Min          Max\n",
    "        0\tPrevious Route Choice        0      total_routes-1\n",
    "        1\tRoute Travel Time            0           +Inf\n",
    "        \n",
    "    Actions:\n",
    "        Type: Discrete(num_of_paths)\n",
    "        Num\tAction\n",
    "        0\tPrecede via Path 1\n",
    "        1\tPrecede via Path 2\n",
    "        ...\n",
    "        n\tPrecede via Path n\n",
    "            \n",
    "    Reward:\n",
    "        Reward for each car is determined by the following formula: \n",
    "        marginal_cost = d[x_e*t(x_e)]/d[x_e]\n",
    "        Cost = (1-λ)route_travel_time + λ(marginal_cost)\n",
    "        Reward = -Cost\n",
    "        ***\n",
    "        route_travel_time: Travel time of the route previously taken by the car\n",
    "        marginal_cost: Cost that the car's route choice imposes on everyone else. \n",
    "                       The formula above captures the change in the travel flow \n",
    "                       with respect to the change in vehicle flow on a given road.\n",
    "        λ: Weight Toward Social Good\n",
    "        \n",
    "    \n",
    "    Starting State:\n",
    "        All observations are assigned -1 for path choice and travel times.\n",
    "    \n",
    "    Episode Termination:\n",
    "        Cars keeps a consistent routing distribution.\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average travel time is less than or equal to the theorical social optimum. \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.network_name = config['network']\n",
    "        self.agents = [gym.make(Routing_Env) for _ in range(config['num_vehicles'])]\n",
    "        self.dones = set()\n",
    "        self.observation_space = self.agents[0].observation_space\n",
    "        self.action_space = self.agents[0].action_space\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        # Create initial observations for each vehicle\n",
    "        prev_path = -1\n",
    "        prev_time = -1\n",
    "        self.state = {i: [prev_path, prev_time] for i in range(self.num_veh)}\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        obs, rew, done, info, costs = {}, {}, {}, {}, {}\n",
    "        # Apply the actions to the network \n",
    "        for i, action in action_dict.items():\n",
    "            # CHANGE THE FOLLOWING!!\n",
    "            costs[i]['path_cost'] = action\n",
    "            costs[i]['marginal_cost'] = 0\n",
    "\n",
    "        # Step the environment\n",
    "        for i, action in action_dict.items():\n",
    "            obs[i], rew[i], done[i], info[i] = self.agents[i].step_after_action_applied(action, \n",
    "                                                                                        path_cost=costs[i]['path_cost'], \n",
    "                                                                                        marginal_cost=costs[i]['marginal_cost'])\n",
    "            if done[i]:\n",
    "                self.dones.add(i)\n",
    "        done[\"__all__\"] = len(self.dones) == len(self.agents)\n",
    "        return obs, rew, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: MultiAgentRouting(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'DQN',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 10\n",
    "            },\n",
    "            'config': {\n",
    "                'env_config': {\n",
    "                    'network': 'Braess',\n",
    "                    'num_vehicles': 1,\n",
    "                    'num_paths': 2,\n",
    "                    'comm': False\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2018-11-26_14-10-25_44054/logs.\n",
      "Waiting for redis server at 127.0.0.1:45123 to respond...\n",
      "Waiting for redis server at 127.0.0.1:55233 to respond...\n",
      "Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8892/notebooks/ray_ui.ipynb?token=f2ae5821200f09cc33e954ded617b94e55cac889f13c1861\n",
      "======================================================================\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 12.8/17.2 GB\n",
      "\n",
      "Created LogSyncer for /Users/mtgibson/ray_results/demo/DQN_corridor_0_2018-11-26_14-10-26e5fckyv0 -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 12.8/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/demo\n",
      "RUNNING trials:\n",
      " - DQN_corridor_0:\tRUNNING\n",
      "\n",
      "Remote function \u001b[31m__init__\u001b[39m failed with:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 848, in _process_task\n",
      "    *arguments)\n",
      "  File \"/Users/mtgibson/ray/python/ray/function_manager.py\", line 481, in actor_method_executor\n",
      "    method_returns = method(actor, *args)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 297, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/Users/mtgibson/ray/python/ray/tune/trainable.py\", line 87, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 345, in _setup\n",
      "    self._init()\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/dqn/dqn.py\", line 144, in _init\n",
      "    self.env_creator, self._policy_graph)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 187, in make_local_evaluator\n",
      "    config[\"local_evaluator_tf_session_args\"]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 235, in _make_evaluator\n",
      "    callbacks=config[\"callbacks\"])\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/evaluation/policy_evaluator.py\", line 269, in __init__\n",
      "    self.env, make_env=make_env, num_envs=num_envs)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 72, in wrap_async\n",
      "    make_env=make_env, existing_envs=[env], num_envs=num_envs)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 276, in __init__\n",
      "    self.env_states = [_MultiAgentEnvState(env) for env in self.envs]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 276, in <listcomp>\n",
      "    self.env_states = [_MultiAgentEnvState(env) for env in self.envs]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 310, in __init__\n",
      "    self.reset()\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 331, in reset\n",
      "    for agent_id in self.last_obs.keys()\n",
      "AttributeError: 'list' object has no attribute 'keys'\n",
      "\n",
      "Remote function \u001b[31mtrain\u001b[39m failed with:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 825, in _process_task\n",
      "    self.reraise_actor_init_error()\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 264, in reraise_actor_init_error\n",
      "    raise self.actor_init_error\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 848, in _process_task\n",
      "    *arguments)\n",
      "  File \"/Users/mtgibson/ray/python/ray/function_manager.py\", line 481, in actor_method_executor\n",
      "    method_returns = method(actor, *args)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 297, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/Users/mtgibson/ray/python/ray/tune/trainable.py\", line 87, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 345, in _setup\n",
      "    self._init()\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/dqn/dqn.py\", line 144, in _init\n",
      "    self.env_creator, self._policy_graph)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 187, in make_local_evaluator\n",
      "    config[\"local_evaluator_tf_session_args\"]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 235, in _make_evaluator\n",
      "    callbacks=config[\"callbacks\"])\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/evaluation/policy_evaluator.py\", line 269, in __init__\n",
      "    self.env, make_env=make_env, num_envs=num_envs)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 72, in wrap_async\n",
      "    make_env=make_env, existing_envs=[env], num_envs=num_envs)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 276, in __init__\n",
      "    self.env_states = [_MultiAgentEnvState(env) for env in self.envs]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 276, in <listcomp>\n",
      "    self.env_states = [_MultiAgentEnvState(env) for env in self.envs]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 310, in __init__\n",
      "    self.reset()\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 331, in reset\n",
      "    for agent_id in self.last_obs.keys()\n",
      "AttributeError: 'list' object has no attribute 'keys'\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mtgibson/ray/python/ray/tune/trial_runner.py\", line 260, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mtgibson/ray/python/ray/tune/ray_trial_executor.py\", line 202, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 2361, in get\n",
      "    raise RayGetError(object_ids, value)\n",
      "ray.worker.RayGetError: Could not get objectid ObjectID(010000008cc5dfa91efdf473219a80548a194313). It was created by remote function \u001b[31mtrain\u001b[39m which failed with:\n",
      "\n",
      "Remote function \u001b[31mtrain\u001b[39m failed with:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 825, in _process_task\n",
      "    self.reraise_actor_init_error()\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 264, in reraise_actor_init_error\n",
      "    raise self.actor_init_error\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 848, in _process_task\n",
      "    *arguments)\n",
      "  File \"/Users/mtgibson/ray/python/ray/function_manager.py\", line 481, in actor_method_executor\n",
      "    method_returns = method(actor, *args)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 297, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/Users/mtgibson/ray/python/ray/tune/trainable.py\", line 87, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 345, in _setup\n",
      "    self._init()\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/dqn/dqn.py\", line 144, in _init\n",
      "    self.env_creator, self._policy_graph)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 187, in make_local_evaluator\n",
      "    config[\"local_evaluator_tf_session_args\"]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 235, in _make_evaluator\n",
      "    callbacks=config[\"callbacks\"])\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/evaluation/policy_evaluator.py\", line 269, in __init__\n",
      "    self.env, make_env=make_env, num_envs=num_envs)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 72, in wrap_async\n",
      "    make_env=make_env, existing_envs=[env], num_envs=num_envs)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 276, in __init__\n",
      "    self.env_states = [_MultiAgentEnvState(env) for env in self.envs]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 276, in <listcomp>\n",
      "    self.env_states = [_MultiAgentEnvState(env) for env in self.envs]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 310, in __init__\n",
      "    self.reset()\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 331, in reset\n",
      "    for agent_id in self.last_obs.keys()\n",
      "AttributeError: 'list' object has no attribute 'keys'\n",
      "\n",
      "Remote function \u001b[31mstop\u001b[39m failed with:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 825, in _process_task\n",
      "    self.reraise_actor_init_error()\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 264, in reraise_actor_init_error\n",
      "    raise self.actor_init_error\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 825, in _process_task\n",
      "    self.reraise_actor_init_error()\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 264, in reraise_actor_init_error\n",
      "    raise self.actor_init_error\n",
      "  File \"/Users/mtgibson/ray/python/ray/worker.py\", line 848, in _process_task\n",
      "    *arguments)\n",
      "  File \"/Users/mtgibson/ray/python/ray/function_manager.py\", line 481, in actor_method_executor\n",
      "    method_returns = method(actor, *args)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 297, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/Users/mtgibson/ray/python/ray/tune/trainable.py\", line 87, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 345, in _setup\n",
      "    self._init()\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/dqn/dqn.py\", line 144, in _init\n",
      "    self.env_creator, self._policy_graph)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 187, in make_local_evaluator\n",
      "    config[\"local_evaluator_tf_session_args\"]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/agents/agent.py\", line 235, in _make_evaluator\n",
      "    callbacks=config[\"callbacks\"])\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/evaluation/policy_evaluator.py\", line 269, in __init__\n",
      "    self.env, make_env=make_env, num_envs=num_envs)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 72, in wrap_async\n",
      "    make_env=make_env, existing_envs=[env], num_envs=num_envs)\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 276, in __init__\n",
      "    self.env_states = [_MultiAgentEnvState(env) for env in self.envs]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 276, in <listcomp>\n",
      "    self.env_states = [_MultiAgentEnvState(env) for env in self.envs]\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 310, in __init__\n",
      "    self.reset()\n",
      "  File \"/Users/mtgibson/ray/python/ray/rllib/env/async_vector_env.py\", line 331, in reset\n",
      "    for agent_id in self.last_obs.keys()\n",
      "AttributeError: 'list' object has no attribute 'keys'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Worker ip unknown, skipping log sync for /Users/mtgibson/ray_results/demo/DQN_corridor_0_2018-11-26_14-10-26e5fckyv0\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.9/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/demo\n",
      "ERROR trials:\n",
      " - DQN_corridor_0:\tERROR, 1 failures: /Users/mtgibson/ray_results/demo/DQN_corridor_0_2018-11-26_14-10-26e5fckyv0/error_2018-11-26_14-10-34.txt\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 11.9/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/demo\n",
      "ERROR trials:\n",
      " - DQN_corridor_0:\tERROR, 1 failures: /Users/mtgibson/ray_results/demo/DQN_corridor_0_2018-11-26_14-10-26e5fckyv0/error_2018-11-26_14-10-34.txt\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [DQN_corridor_0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ab68b567c27c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m             \"config\": {\n\u001b[1;32m     88\u001b[0m                 \"env_config\": {\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;34m\"corridor_length\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 },\n\u001b[1;32m     91\u001b[0m             },\n",
      "\u001b[0;32m~/ray/python/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [DQN_corridor_0])"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import ray\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(0.0, self.end_pos, shape=(1, ), dtype=np.float32)\n",
    "        self._spec = EnvSpec(\"SimpleCorridor-{}-v0\".format(self.end_pos))\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        return [self.cur_pos], 1 if done else 0, done, {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = \"corridor\"\n",
    "    register_env(env_creator_name, lambda config: SimpleCorridor(config))\n",
    "    ray.init()\n",
    "    run_experiments({\n",
    "        \"demo\": {\n",
    "            \"run\": \"DQN\",\n",
    "            \"env\": \"corridor\",\n",
    "            \"config\": {\n",
    "                \"env_config\": {\n",
    "                    \"corridor_length\": 5,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
