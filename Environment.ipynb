{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KS6wwFRc9CQK"
   },
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cslVfkBSLean"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import Networks\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_policy_graph import *\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import *\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.models.preprocessors import DictFlatteningPreprocessor, Preprocessor\n",
    "\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8YRoyVY687z"
   },
   "source": [
    "### Environment Below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73fz3Ogq6870"
   },
   "outputs": [],
   "source": [
    "gamma = 0\n",
    "\n",
    "def reward_calculator(travel_time, marginal_cost, soc_fac):\n",
    "    # ADD THE BREMIAN DIVERGENCE!!!\n",
    "    rew_dict = {}\n",
    "    total_travel_time = 0\n",
    "    for p, tt in travel_time.items():\n",
    "        total_travel_time += tt\n",
    "    for agent in travel_time.keys():\n",
    "        rew_dict[agent] = - (travel_time[agent] + soc_fac * marginal_cost[agent])\n",
    "        # rew_dict[agent] = - total_travel_time\n",
    "    return rew_dict\n",
    "\n",
    "## Routing Environment \n",
    "class RoutingEnv(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        The cars start at the same origin point, Point A, and need to reach the same destination, Point Z. \n",
    "        Each car can reach Point Z via a variety of routing choices described in a given network, in which \n",
    "        each route introduces different travel times and congestion. \n",
    "        The goal is to minimize the average travel times amongst each of the cars.\n",
    "    \n",
    "    Observation: \n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tObservation                 Min          Max\n",
    "        0\tPrevious Route Choice        0      total_routes-1\n",
    "        1\tRoute Travel Time            0           +Inf\n",
    "        2\tComm Message               -Inf          +Inf\n",
    "        \n",
    "    Actions:\n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tAction                      Min          Max\n",
    "        0\tFuture Path_Choice           0      total_routes-1\n",
    "        1\tComm Message               -Inf          +Inf\n",
    "            \n",
    "    Reward:\n",
    "        Reward for each car is determined by the following formula: \n",
    "        marginal_cost = d[t(x_e)]/d[x_e]\n",
    "        Cost = route_travel_time + λ(marginal_cost)\n",
    "        Reward = -Cost\n",
    "        ***\n",
    "        route_travel_time: Travel time of the route previously taken by the car\n",
    "        marginal_cost: Cost that the car's route choice imposes on everyone else. \n",
    "                       The formula above captures the change in the travel flow \n",
    "                       with respect to the change in vehicle flow on a given road.\n",
    "        λ: Weight Toward Social Good (between 0 and 1)\n",
    "        \n",
    "    \n",
    "    Starting State:\n",
    "        All observations are assigned -1 for path choice and travel times.\n",
    "    \n",
    "    Episode Termination:\n",
    "        Cars keeps a consistent routing distribution.\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average travel time is less than or equal to the theorical social optimum. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        self.network_name = config['network']\n",
    "        self.num_paths = config['num_paths']\n",
    "        self.soc_fac = config['soc_fac']\n",
    "        self.num_veh = config['num_veh']\n",
    "        self.num_obs = 2\n",
    "        self.num_actions = 1\n",
    "        self.state = None\n",
    "        # Make observation space\n",
    "        obs_spaces = {\n",
    "            ############ TO DO: Change the observation space #############\n",
    "            'prev_route': Discrete(self.num_paths),\n",
    "            'prev_time': Box(low=0, \n",
    "                             high=float('+inf'), \n",
    "                             shape=(1,), \n",
    "                             dtype=np.float32),\n",
    "            'best_route': Discrete(self.num_paths),\n",
    "            'best_time': Box(low=0, \n",
    "                             high=float('+inf'), \n",
    "                             shape=(1,), \n",
    "                             dtype=np.float32)\n",
    "        }\n",
    "        self.preprocessor = DictFlatteningPreprocessor(Dict(obs_spaces))\n",
    "        self.observation_space = self.preprocessor.observation_space\n",
    "        # Make the action space\n",
    "        self.action_space = Discrete(self.num_paths) # int between 0 and num_paths-1\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        # Create initial observations for each vehicle\n",
    "        start = {\n",
    "            ############ TO DO: Change the observation space #############\n",
    "            'prev_route': 0,\n",
    "            'prev_time': 0,\n",
    "            'best_route': 0,\n",
    "            'best_time': 0\n",
    "        }\n",
    "        self.state = {'car_{}'.format(i): self.preprocessor.transform(start) for i in range(self.num_veh)}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        ### ADD THE COMMUNICATION CHANNEL\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        # Apply the actions of every agent at the same time\n",
    "        paths_flow_dict = {}\n",
    "        \n",
    "        for agent, rl_action in action_dict.items():\n",
    "            # agent is one string that represent the id of the agent\n",
    "            # rl_action is one number that represent the path choice of the agent,\n",
    "            # rl_action should be a int between 0 and nb_paths-1\n",
    "            rl_action = int(rl_action)\n",
    "            assert type(rl_action) == int and rl_action > -1 and rl_action < network.nb_paths\n",
    "            # we built a dictionnary paths_flow_dict that store the path flow on every path\n",
    "            if rl_action in paths_flow_dict:\n",
    "                paths_flow_dict[rl_action] += 1\n",
    "            else:\n",
    "                paths_flow_dict[rl_action] = 1\n",
    "\n",
    "        # update the path travel times of the network given the path flows\n",
    "        network.update_flow_from_dict(paths_flow_dict)\n",
    "        \n",
    "        \n",
    "        # Calculate states, reward, and done for each agent\n",
    "        travel_time = {}\n",
    "        marginal_cost = {}\n",
    "        \n",
    "        tt_array = np.zeros(nb_path)\n",
    "        for p in range(nb_path):\n",
    "            tt_array = network.travel_time(p)\n",
    "        best_path = np.argmin(tt_array)\n",
    "        best_travel_time = np.min(tt_array)\n",
    "        \n",
    "        for agent, path_choice in action_dict.items():\n",
    "            path_choice = int(path_choice)\n",
    "            assert type(path_choice) == int and path_choice > -1 and path_choice < network.nb_paths\n",
    "            # network travel time ( path ) return the travel time of the path\n",
    "            travel_time[agent] = network.travel_time(path_choice)\n",
    "            # network marginal cost ( path ) return the marginal cost of the path\n",
    "            marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "            new_obs = {\n",
    "                ############ TO DO: Change the observation space #############\n",
    "                'prev_route': path_choice,\n",
    "                'prev_time': travel_time[agent],\n",
    "                'best_route': best_path,\n",
    "                'best_time': best_travel_time\n",
    "            }\n",
    "            obs_dict[agent] = self.preprocessor.transform(new_obs)\n",
    "            # Cost is the path_time\n",
    "            # rew_dict[agent] = reward_calculator(agent, marginal_cost)\n",
    "            # -path_choice # TO-DO: CHANGE THIS! \n",
    "            # Set done and infos\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "        rew_dict = reward_calculator(travel_time, marginal_cost, self.soc_fac)\n",
    "        self.state = obs_dict   \n",
    "        \n",
    "        self.file = open(\"/Users/theophile/Documents/Classes/FLOW/Project/learning_wardrop/test_lambda_\" + str(self.soc_fac) + \"_gamma_\" + str(gamma) + \"_know_best_path_PPO\", 'a')\n",
    "        self.file.write(\"Actions: \" + str(action_dict) + '\\n')\n",
    "        self.file.write(\"Reward: \" + str(rew_dict) + '\\n')\n",
    "        self.file.close()\n",
    "        \n",
    "        done[\"__all__\"] = True\n",
    "         \n",
    "        return obs_dict, rew_dict, done, info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code runs the experiment for the multiagent problem.\n",
    "\n",
    "Remark:\n",
    "On the Braess network using 4 vehicles, we should get:\n",
    "- if the social factor is 0, Nash: a reward of -3.75 in average, 2 cars on the first path, 1 on the second and third path\n",
    "- if the social factor is 1, Social optimum: a travel time of -3.5 in average (a reward of ), 2 cars on the first path, 1 on the second and third path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2018-12-06_16-27-56_5352/logs.\n",
      "Waiting for redis server at 127.0.0.1:62725 to respond...\n",
      "Waiting for redis server at 127.0.0.1:42075 to respond...\n",
      "Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=e60d439cf71e428514a36a5eb4bff7526f6f568649706d6e\n",
      "======================================================================\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.1/17.2 GB\n",
      "\n",
      "Created LogSyncer for /Users/theophile/ray_results/route-DQN/PPO_multi_routing_0_2018-12-06_16-27-563l_7m9x3 -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.1/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - PPO_multi_routing_0:\tRUNNING\n",
      "\n",
      "A worker died or was killed while executing task 00000000de9a5540c5dd581fa9e0ea7f4931f786.\n",
      "A worker died or was killed while executing task 00000000957a832ab691035179d0ba9aadb35e29.\n",
      "Remote function \u001b[31mtrain\u001b[39m failed with:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/worker.py\", line 856, in _process_task\n",
      "    *arguments)\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/function_manager.py\", line 481, in actor_method_executor\n",
      "    method_returns = method(actor, *args)\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/rllib/agents/agent.py\", line 319, in train\n",
      "    return Trainable.train(self)\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/rllib/agents/ppo/ppo.py\", line 122, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/rllib/optimizers/sync_samples_optimizer.py\", line 45, in step\n",
      "    e.sample.remote() for e in self.remote_evaluators\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/worker.py\", line 2358, in get\n",
      "    raise RayGetError(object_ids[i], value)\n",
      "ray.worker.RayGetError: Could not get objectid ObjectID(01000000957a832ab691035179d0ba9aadb35e29). It was created by remote function \u001b[31m<unknown>\u001b[39m which failed with:\n",
      "\n",
      "Remote function \u001b[31m<unknown>\u001b[39m failed with:\n",
      "\n",
      "Invalid return value: likely worker died or was killed while executing the task; check previous logs or dmesg for errors.\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/tune/ray_trial_executor.py\", line 202, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/worker.py\", line 2366, in get\n",
      "    raise RayGetError(object_ids, value)\n",
      "ray.worker.RayGetError: Could not get objectid ObjectID(0100000020d23652fd4c82c7f042c6a9f6aca01d). It was created by remote function \u001b[31mtrain\u001b[39m which failed with:\n",
      "\n",
      "Remote function \u001b[31mtrain\u001b[39m failed with:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/worker.py\", line 856, in _process_task\n",
      "    *arguments)\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/function_manager.py\", line 481, in actor_method_executor\n",
      "    method_returns = method(actor, *args)\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/rllib/agents/agent.py\", line 319, in train\n",
      "    return Trainable.train(self)\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/rllib/agents/ppo/ppo.py\", line 122, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/rllib/optimizers/sync_samples_optimizer.py\", line 45, in step\n",
      "    e.sample.remote() for e in self.remote_evaluators\n",
      "  File \"/Users/theophile/Documents/Classes/FLOW/Project/ray/python/ray/worker.py\", line 2358, in get\n",
      "    raise RayGetError(object_ids[i], value)\n",
      "ray.worker.RayGetError: Could not get objectid ObjectID(01000000957a832ab691035179d0ba9aadb35e29). It was created by remote function \u001b[31m<unknown>\u001b[39m which failed with:\n",
      "\n",
      "Remote function \u001b[31m<unknown>\u001b[39m failed with:\n",
      "\n",
      "Invalid return value: likely worker died or was killed while executing the task; check previous logs or dmesg for errors.\n",
      "\n",
      "Worker ip unknown, skipping log sync for /Users/theophile/ray_results/route-DQN/PPO_multi_routing_0_2018-12-06_16-27-563l_7m9x3\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.1/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "ERROR trials:\n",
      " - PPO_multi_routing_0:\tERROR, 1 failures: /Users/theophile/ray_results/route-DQN/PPO_multi_routing_0_2018-12-06_16-27-563l_7m9x3/error_2018-12-06_16-28-14.txt\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.1/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "ERROR trials:\n",
      " - PPO_multi_routing_0:\tERROR, 1 failures: /Users/theophile/ray_results/route-DQN/PPO_multi_routing_0_2018-12-06_16-27-563l_7m9x3/error_2018-12-06_16-28-14.txt\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_multi_routing_0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2ea02d0ac9e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     }\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Classes/FLOW/Project/ray/python/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_multi_routing_0])"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup policies for each vehicle\n",
    "\n",
    "# to change\n",
    "gamma = 0.5\n",
    "lambd = 0\n",
    "\n",
    "network_name = 'Braess'\n",
    "nb_veh = 4\n",
    "# network init should build the Network object\n",
    "# ----- TO DO -----: import the network class\n",
    "network = Networks.network(network_name, nb_veh)\n",
    "# nb_path should be a property method\n",
    "\n",
    "\"\"\"\n",
    "define a function (class instantiation) which have for parameter a network name, \n",
    "and the number of vehicles, \n",
    "the return the num paths.\n",
    "\n",
    "interface of the class network\n",
    "class network:\n",
    "    def __init__(self, network_name, nb_veh):\n",
    "        load the network which correspond to the network_name\n",
    "        define the nb_veh as the nb_veh\n",
    "        from nb_veh and the intern demand define the number of flow that each veh represent\n",
    "        also define __nb_paths to give it to the Env\n",
    "\n",
    "    @property\n",
    "    def nb_paths(self):\n",
    "        return self.__nb_paths\n",
    "\"\"\"\n",
    "\n",
    "nb_path = network.nb_paths\n",
    "\n",
    "env_config = {\n",
    "    'network': network_name,\n",
    "    'num_veh': nb_veh,\n",
    "    'num_paths': nb_path,\n",
    "    'soc_fac': lambd\n",
    "}\n",
    "routing_env = RoutingEnv(env_config)\n",
    "car_obs_space = routing_env.observation_space\n",
    "car_act_space = routing_env.action_space\n",
    "config = {\"gamma\": gamma}\n",
    "policy_graphs = {\n",
    "    'vehicles': (PPOPolicyGraph, car_obs_space, car_act_space, config)\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: RoutingEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'PPO',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 100\n",
    "            },\n",
    "            'config': {\n",
    "                'env_config': env_config,\n",
    "                'multiagent': {\n",
    "                    'policy_graphs': policy_graphs,\n",
    "                    'policy_mapping_fn': tune.function(lambda agent_id: 'vehicles')\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "### First, we use the travel time of the last iteration as the reward\n",
    "$\\gamma = 0$ and $\\lambda = 0$.\n",
    "We should obtain the nash equilibrium:\n",
    "Where the travel time are $[3.75, 3.75, 3.75]$\n",
    "\n",
    "### Then, we use marginal cost of the last iteration as the reward\n",
    "$\\gamma = 0$ and $\\lambda = 1$.\n",
    "We should obtain the social equilibrium:\n",
    "Where the travel time are $[3.25, 3.5, 3.5]$\n",
    "\n",
    "This is not the case here. It seems that the cars learn to do all the same thing.\n",
    "\n",
    "### We should try to add the bregman divergence in the reward (cf Walid, with a $\\gamma = 1$ (no regret online learning).\n",
    "### We should try to understand the learning process of the agents\n",
    "### We should also try to change the discount factor $\\gamma$\n",
    "### We should also add the communication process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tt_mc(action_dict, network, soc_fact):\n",
    "    paths_flow_dict = {}\n",
    "    for agent, rl_action in action_dict.items():\n",
    "        rl_action = int(rl_action)\n",
    "        if rl_action in paths_flow_dict:\n",
    "            paths_flow_dict[rl_action] += 1\n",
    "        else:\n",
    "            paths_flow_dict[rl_action] = 1\n",
    "    network.update_flow_from_dict(paths_flow_dict)\n",
    "\n",
    "    travel_time = {}\n",
    "    marginal_cost = {}\n",
    "    for agent, path_choice in action_dict.items():\n",
    "        path_choice = int(path_choice)\n",
    "        travel_time[agent] = network.travel_time(path_choice)\n",
    "        marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "    rew_dict = reward_calculator(travel_time, marginal_cost, soc_fact)\n",
    "    return travel_time, marginal_cost, rew_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "file = open(\"/Users/theophile/Documents/Classes/FLOW/Project/learning_wardrop/test_lambda_0_gamma_0.5_know_best_path_DQN\", 'r')\n",
    "j = 0\n",
    "# we want to plot the evolution of the path choice, of the reward and of the travel time\n",
    "Actions_plot = np.array([[0, 0, 0, 0]])\n",
    "Reward_plot = np.array([[0, 0, 0, 0]])\n",
    "Travel_time_plot = np.array([[0, 0, 0, 0]])\n",
    "while(True):\n",
    "    j = j+1\n",
    "    actions = file.readline()\n",
    "    rewards = file.readline()\n",
    "    if j%100 != 1:\n",
    "        continue\n",
    "    try:\n",
    "        action_dict = ast.literal_eval(\"{\" + actions.split('{')[1].split('}')[0]+ \"}\")\n",
    "        reward_dict = ast.literal_eval(\"{\" + rewards.split('{')[1].split('}')[0]+ \"}\")\n",
    "    \n",
    "        network = Networks.network(network_name, nb_veh)\n",
    "        travel_time, marginal_cost, rew_dict = get_tt_mc(action_dict, network, 0)\n",
    "        \n",
    "        actions_np = np.fromiter(action_dict.values(), dtype=int)\n",
    "        Actions_plot = np.append(Actions_plot, [actions_np], axis=0)\n",
    "        rewards_np = np.fromiter(reward_dict.values(), dtype=float)\n",
    "        Reward_plot = np.append(Reward_plot, [rewards_np], axis=0)\n",
    "        travel_time_np = np.fromiter(travel_time.values(), dtype=float)\n",
    "        Travel_time_plot = np.append(Travel_time_plot, [travel_time_np], axis=0)\n",
    "        if(j==1):\n",
    "            print(\"------ First iteration ------\")\n",
    "            print(\"Path choice: \" + str(action_dict))\n",
    "            print(\"Reward ray: \" + str(reward_dict))\n",
    "            print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "            print(\"Travel time cars: \" + str(travel_time))\n",
    "            print(\"Marginal cost: \" + str(marginal_cost))\n",
    "            print(\"Reward network: \" + str(rew_dict))\n",
    "    except:\n",
    "        print()\n",
    "        print(\"------ Last iteration ------\")\n",
    "        print(\"Path choice: \" + str(action_dict))\n",
    "        print(\"Reward ray: \" + str(reward_dict))\n",
    "        print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "        print(\"Travel time cars: \" + str(travel_time))\n",
    "        print(\"Marginal cost: \" + str(marginal_cost))\n",
    "        print(\"Reward network: \" + str(rew_dict))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(4):\n",
    "    plt.plot(Actions_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Path choice of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(Reward_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Rewards of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(Travel_time_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Travel time of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Environment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
