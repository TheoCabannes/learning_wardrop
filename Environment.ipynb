{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KS6wwFRc9CQK"
   },
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cslVfkBSLean"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import Networks\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_policy_graph import *\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.models.preprocessors import DictFlatteningPreprocessor, Preprocessor\n",
    "\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8YRoyVY687z"
   },
   "source": [
    "### Environment Below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73fz3Ogq6870"
   },
   "outputs": [],
   "source": [
    "path_choice_saver = {}\n",
    "def reward_calculator(travel_time, marginal_cost, soc_fac):\n",
    "    rew_dict = {}\n",
    "    for agent in travel_time.keys():\n",
    "        rew_dict[agent] = - (travel_time[agent] + soc_fac * marginal_cost[agent])\n",
    "    return rew_dict\n",
    "\n",
    "## Routing Environment \n",
    "class RoutingEnv(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        The cars start at the same origin point, Point A, and need to reach the same destination, Point Z. \n",
    "        Each car can reach Point Z via a variety of routing choices described in a given network, in which \n",
    "        each route introduces different travel times and congestion. \n",
    "        The goal is to minimize the average travel times amongst each of the cars.\n",
    "    \n",
    "    Observation: \n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tObservation                 Min          Max\n",
    "        0\tPrevious Route Choice        0      total_routes-1\n",
    "        1\tRoute Travel Time            0           +Inf\n",
    "        2\tComm Message               -Inf          +Inf\n",
    "        \n",
    "    Actions:\n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tAction                      Min          Max\n",
    "        0\tFuture Path_Choice           0      total_routes-1\n",
    "        1\tComm Message               -Inf          +Inf\n",
    "            \n",
    "    Reward:\n",
    "        Reward for each car is determined by the following formula: \n",
    "        marginal_cost = d[t(x_e)]/d[x_e]\n",
    "        Cost = route_travel_time + λ(marginal_cost)\n",
    "        Reward = -Cost\n",
    "        ***\n",
    "        route_travel_time: Travel time of the route previously taken by the car\n",
    "        marginal_cost: Cost that the car's route choice imposes on everyone else. \n",
    "                       The formula above captures the change in the travel flow \n",
    "                       with respect to the change in vehicle flow on a given road.\n",
    "        λ: Weight Toward Social Good (between 0 and 1)\n",
    "        \n",
    "    \n",
    "    Starting State:\n",
    "        All observations are assigned -1 for path choice and travel times.\n",
    "    \n",
    "    Episode Termination:\n",
    "        Cars keeps a consistent routing distribution.\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average travel time is less than or equal to the theorical social optimum. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        self.network_name = config['network']\n",
    "        self.num_paths = config['num_paths']\n",
    "        self.soc_fac = config['soc_fac']\n",
    "        self.num_veh = config['num_veh']\n",
    "        self.num_obs = 2\n",
    "        self.num_actions = 1\n",
    "        self.state = None\n",
    "        # Make observation space\n",
    "        obs_spaces = {\n",
    "            'prev_route': Discrete(self.num_paths),\n",
    "            'prev_time': Box(low=0, \n",
    "                             high=float('+inf'), \n",
    "                             shape=(1,), \n",
    "                             dtype=np.float32)\n",
    "        }\n",
    "        self.preprocessor = DictFlatteningPreprocessor(Dict(obs_spaces))\n",
    "        self.observation_space = self.preprocessor.observation_space\n",
    "        # Make the action space\n",
    "        self.action_space = Discrete(self.num_paths) # int between 0 and num_paths-1\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        # Create initial observations for each vehicle\n",
    "        start = {\n",
    "            'prev_route': 0,\n",
    "            'prev_time': 0\n",
    "        }\n",
    "        self.state = {'car_{}'.format(i): self.preprocessor.transform(start) for i in range(self.num_veh)}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        # Apply the actions of every agent at the same time\n",
    "        paths_flow_dict = {}\n",
    "        for agent, rl_action in action_dict.items():\n",
    "            # agent is one string that represent the id of the agent\n",
    "            # rl_action is one number that represent the path choice of the agent,\n",
    "            # rl_action should be a int between 0 and nb_paths-1\n",
    "            rl_action = int(rl_action)\n",
    "            # print(str(agent) + \" is on path \" + str(rl_action))\n",
    "            assert type(rl_action) == int and rl_action > -1 and rl_action < network.nb_paths\n",
    "            # we built a dictionnary paths_flow_dict that store the path flow on every path\n",
    "            if rl_action in paths_flow_dict:\n",
    "                paths_flow_dict[rl_action] += 1\n",
    "            else:\n",
    "                paths_flow_dict[rl_action] = 1\n",
    "\n",
    "        # update the path travel times of the network given the path flows\n",
    "        network.update_flow_from_dict(paths_flow_dict)\n",
    "        \n",
    "        \n",
    "        # Calculate states, reward, and done for each agent\n",
    "        travel_time = {}\n",
    "        marginal_cost = {}\n",
    "        \n",
    "        for agent, path_choice in action_dict.items():\n",
    "            path_choice = int(path_choice)\n",
    "            assert type(path_choice) == int and path_choice > -1 and path_choice < network.nb_paths\n",
    "            # network travel time ( path ) return the travel time of the path\n",
    "            travel_time[agent] = network.travel_time(path_choice)\n",
    "            # network marginal cost ( path ) return the marginal cost of the path\n",
    "            marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "            new_obs = {\n",
    "                'prev_route': path_choice,\n",
    "                'prev_time': travel_time[agent]\n",
    "            }\n",
    "            obs_dict[agent] = self.preprocessor.transform(new_obs)\n",
    "            # Cost is the path_time\n",
    "            # rew_dict[agent] = reward_calculator(agent, marginal_cost)\n",
    "            # -path_choice # TO-DO: CHANGE THIS! \n",
    "            # Set done and infos\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "        rew_dict = reward_calculator(travel_time, marginal_cost, self.soc_fac)\n",
    "        self.state = obs_dict   \n",
    "        done[\"__all__\"] = True\n",
    "         \n",
    "        return obs_dict, rew_dict, done, info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code runs the experiment for the multiagent problem.\n",
    "\n",
    "Remark:\n",
    "On the Braess network using 4 vehicles, we should get:\n",
    "- if the social factor is 0, Nash: a reward of -3.75 in average, 2 cars on the first path, 1 on the second and third path\n",
    "- if the social factor is 1, Social optimum: a travel time of -3.5 in average (a reward of ), 2 cars on the first path, 1 on the second and third path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2018-11-30_16-26-43_957/logs.\n",
      "Waiting for redis server at 127.0.0.1:20392 to respond...\n",
      "Waiting for redis server at 127.0.0.1:17150 to respond...\n",
      "Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=0db2fca1489f5c89f5457520945cfc006380996d3199c823\n",
      "======================================================================\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/17.2 GB\n",
      "\n",
      "Created LogSyncer for /Users/theophile/ray_results/route-DQN/DQN_multi_routing_0_2018-11-30_16-26-44t5cygmqf -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.3/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -15.537799999999999\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 9.817\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.8844500000000006\n",
      "  time_since_restore: 2.63688325881958\n",
      "  time_this_iter_s: 2.63688325881958\n",
      "  time_total_s: 2.63688325881958\n",
      "  timestamp: 1543624017\n",
      "  timesteps_since_restore: 1000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 2 s, 1 iter, 1000 ts, -15.5 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-03\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -15.450700000000001\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 2000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.158\n",
      "    max_exploration: 0.902\n",
      "    min_exploration: 0.902\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8000\n",
      "    num_target_updates: 3\n",
      "    opt_peak_throughput: 4470.661\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.213\n",
      "    sample_time_ms: 11.053\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.8626750000000007\n",
      "  time_since_restore: 8.479931354522705\n",
      "  time_this_iter_s: 5.843048095703125\n",
      "  time_total_s: 8.479931354522705\n",
      "  timestamp: 1543624023\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 8 s, 2 iter, 2000 ts, -15.5 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-08\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -15.4436\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 3000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.795\n",
      "    max_exploration: 0.804\n",
      "    min_exploration: 0.804\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16000\n",
      "    num_target_updates: 5\n",
      "    opt_peak_throughput: 4709.295\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.102\n",
      "    sample_time_ms: 9.892\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.860900000000001\n",
      "  time_since_restore: 13.595011472702026\n",
      "  time_this_iter_s: 5.115080118179321\n",
      "  time_total_s: 13.595011472702026\n",
      "  timestamp: 1543624028\n",
      "  timesteps_since_restore: 3000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 13 s, 3 iter, 3000 ts, -15.4 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-14\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -15.5159\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 4000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.581\n",
      "    max_exploration: 0.706\n",
      "    min_exploration: 0.706\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 24000\n",
      "    num_target_updates: 7\n",
      "    opt_peak_throughput: 4862.397\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.158\n",
      "    sample_time_ms: 10.031\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.878975000000001\n",
      "  time_since_restore: 18.759127855300903\n",
      "  time_this_iter_s: 5.164116382598877\n",
      "  time_total_s: 18.759127855300903\n",
      "  timestamp: 1543624034\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 18 s, 4 iter, 4000 ts, -15.5 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-19\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -15.5441\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 5000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.674\n",
      "    max_exploration: 0.608\n",
      "    min_exploration: 0.608\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32000\n",
      "    num_target_updates: 9\n",
      "    opt_peak_throughput: 4169.806\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.107\n",
      "    sample_time_ms: 9.961\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.8860250000000005\n",
      "  time_since_restore: 23.845236778259277\n",
      "  time_this_iter_s: 5.086108922958374\n",
      "  time_total_s: 23.845236778259277\n",
      "  timestamp: 1543624039\n",
      "  timesteps_since_restore: 5000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 23 s, 5 iter, 5000 ts, -15.5 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-24\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -15.6357\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 6000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.926\n",
      "    max_exploration: 0.51\n",
      "    min_exploration: 0.51\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 40000\n",
      "    num_target_updates: 11\n",
      "    opt_peak_throughput: 4620.088\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.15\n",
      "    sample_time_ms: 9.885\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.908925000000001\n",
      "  time_since_restore: 28.87858486175537\n",
      "  time_this_iter_s: 5.033348083496094\n",
      "  time_total_s: 28.87858486175537\n",
      "  timestamp: 1543624044\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 28 s, 6 iter, 6000 ts, -15.6 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-29\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -15.736700000000003\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 7000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.228\n",
      "    max_exploration: 0.41200000000000003\n",
      "    min_exploration: 0.41200000000000003\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 48000\n",
      "    num_target_updates: 13\n",
      "    opt_peak_throughput: 4427.13\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.172\n",
      "    sample_time_ms: 10.163\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.934175000000001\n",
      "  time_since_restore: 34.082170724868774\n",
      "  time_this_iter_s: 5.203585863113403\n",
      "  time_total_s: 34.082170724868774\n",
      "  timestamp: 1543624049\n",
      "  timesteps_since_restore: 7000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 34 s, 7 iter, 7000 ts, -15.7 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-34\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -15.870500000000002\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 8000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.38\n",
      "    max_exploration: 0.31400000000000006\n",
      "    min_exploration: 0.31400000000000006\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 56000\n",
      "    num_target_updates: 15\n",
      "    opt_peak_throughput: 4336.234\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.193\n",
      "    sample_time_ms: 10.008\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.967625000000001\n",
      "  time_since_restore: 39.3732647895813\n",
      "  time_this_iter_s: 5.291094064712524\n",
      "  time_total_s: 39.3732647895813\n",
      "  timestamp: 1543624054\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 39 s, 8 iter, 8000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-40\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -16.117\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 9000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.008\n",
      "    max_exploration: 0.21599999999999997\n",
      "    min_exploration: 0.21599999999999997\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 64000\n",
      "    num_target_updates: 17\n",
      "    opt_peak_throughput: 4566.471\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.223\n",
      "    sample_time_ms: 10.529\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.029250000000001\n",
      "  time_since_restore: 44.62860059738159\n",
      "  time_this_iter_s: 5.255335807800293\n",
      "  time_total_s: 44.62860059738159\n",
      "  timestamp: 1543624060\n",
      "  timesteps_since_restore: 9000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=975], 44 s, 9 iter, 9000 ts, -16.1 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_16-27-45\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.4\n",
      "  episode_reward_mean: -16.2921\n",
      "  episode_reward_min: -17.8\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 10000\n",
      "  experiment_id: 2d9440a3726645bbbb5405c5ae70feea\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.997\n",
      "    max_exploration: 0.118\n",
      "    min_exploration: 0.118\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 72000\n",
      "    num_target_updates: 19\n",
      "    opt_peak_throughput: 4573.676\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.042\n",
      "    sample_time_ms: 10.045\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 975\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.073025\n",
      "  time_since_restore: 50.097222566604614\n",
      "  time_this_iter_s: 5.4686219692230225\n",
      "  time_total_s: 50.097222566604614\n",
      "  timestamp: 1543624065\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "TERMINATED trials:\n",
      " - DQN_multi_routing_0:\tTERMINATED [pid=975], 50 s, 10 iter, 10000 ts, -16.3 rew\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.2/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "TERMINATED trials:\n",
      " - DQN_multi_routing_0:\tTERMINATED [pid=975], 50 s, 10 iter, 10000 ts, -16.3 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup policies for each vehicle\n",
    "\n",
    "network_name = 'Braess'\n",
    "nb_veh = 4\n",
    "# network init should build the Network object\n",
    "# ----- TO DO -----: import the network class\n",
    "network = Networks.network(network_name, nb_veh)\n",
    "# nb_path should be a property method\n",
    "\n",
    "\"\"\"\n",
    "define a function (class instantiation) which have for parameter a network name, \n",
    "and the number of vehicles, \n",
    "the return the num paths.\n",
    "\n",
    "interface of the class network\n",
    "class network:\n",
    "    def __init__(self, network_name, nb_veh):\n",
    "        load the network which correspond to the network_name\n",
    "        define the nb_veh as the nb_veh\n",
    "        from nb_veh and the intern demand define the number of flow that each veh represent\n",
    "        also define __nb_paths to give it to the Env\n",
    "\n",
    "    @property\n",
    "    def nb_paths(self):\n",
    "        return self.__nb_paths\n",
    "\"\"\"\n",
    "\n",
    "nb_path = network.nb_paths\n",
    "\n",
    "env_config = {\n",
    "    'network': network_name,\n",
    "    'num_veh': nb_veh,\n",
    "    'num_paths': nb_path,\n",
    "    'soc_fac': 1 # to change\n",
    "}\n",
    "routing_env = RoutingEnv(env_config)\n",
    "car_obs_space = routing_env.observation_space\n",
    "car_act_space = routing_env.action_space\n",
    "config = {\"gamma\": 0.0}\n",
    "policy_graphs = {\n",
    "    'vehicles': (DQNPolicyGraph, car_obs_space, car_act_space, config)\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: RoutingEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'DQN',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 10\n",
    "            },\n",
    "            'config': {\n",
    "                'env_config': env_config,\n",
    "                'multiagent': {\n",
    "                    'policy_graphs': policy_graphs,\n",
    "                    'policy_mapping_fn': tune.function(lambda agent_id: 'vehicles')\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(path_choice_saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Environment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
