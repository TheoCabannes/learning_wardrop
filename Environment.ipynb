{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KS6wwFRc9CQK"
   },
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cslVfkBSLean"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_policy_graph import *\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.models.preprocessors import DictFlatteningPreprocessor, Preprocessor\n",
    "\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8YRoyVY687z"
   },
   "source": [
    "### Environment Below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73fz3Ogq6870"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiAgentEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-caaf15c494b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Routing Environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRoutingEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiAgentEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \"\"\"\n\u001b[1;32m      4\u001b[0m     \u001b[0mINPUT\u001b[0m \u001b[0mLATER\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRight\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mno\u001b[0m \u001b[0mcommunication\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mAnd\u001b[0m \u001b[0mpartial\u001b[0m \u001b[0mobservability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiAgentEnv' is not defined"
     ]
    }
   ],
   "source": [
    "def reward_calculator(travel_time, marginal_cost, soc_fac):\n",
    "    rew_dict = {}\n",
    "    for agent in travel_time.keys():\n",
    "        rew_dict[agent] = travel_time[agent] + soc_fac * marginal_cost[agent]\n",
    "    return rew_dict\n",
    "\n",
    "## Routing Environment \n",
    "class RoutingEnv(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        The cars start at the same origin point, Point A, and need to reach the same destination, Point Z. \n",
    "        Each car can reach Point Z via a variety of routing choices described in a given network, in which \n",
    "        each route introduces different travel times and congestion. \n",
    "        The goal is to minimize the average travel times amongst each of the cars.\n",
    "    \n",
    "    Observation: \n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tObservation                 Min          Max\n",
    "        0\tPrevious Route Choice        0      total_routes-1\n",
    "        1\tRoute Travel Time            0           +Inf\n",
    "        2\tComm Message               -Inf          +Inf\n",
    "        \n",
    "    Actions:\n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tAction                      Min          Max\n",
    "        0\tFuture Path_Choice           0      total_routes-1\n",
    "        1\tComm Message               -Inf          +Inf\n",
    "            \n",
    "    Reward:\n",
    "        Reward for each car is determined by the following formula: \n",
    "        marginal_cost = d[t(x_e)]/d[x_e]\n",
    "        Cost = route_travel_time + λ(marginal_cost)\n",
    "        Reward = -Cost\n",
    "        ***\n",
    "        route_travel_time: Travel time of the route previously taken by the car\n",
    "        marginal_cost: Cost that the car's route choice imposes on everyone else. \n",
    "                       The formula above captures the change in the travel flow \n",
    "                       with respect to the change in vehicle flow on a given road.\n",
    "        λ: Weight Toward Social Good (between 0 and 1)\n",
    "        \n",
    "    \n",
    "    Starting State:\n",
    "        All observations are assigned -1 for path choice and travel times.\n",
    "    \n",
    "    Episode Termination:\n",
    "        Cars keeps a consistent routing distribution.\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average travel time is less than or equal to the theorical social optimum. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        self.network_name = config['network']\n",
    "        self.num_paths = config['num_paths']\n",
    "        self.soc_fac = config['soc_fac']\n",
    "        self.num_veh = config['num_veh']\n",
    "        self.num_obs = 2\n",
    "        self.num_actions = 1\n",
    "        self.state = None\n",
    "        # Make observation space\n",
    "        obs_spaces = {\n",
    "            'prev_route': Discrete(self.num_paths),\n",
    "            'prev_time': Box(low=0, \n",
    "                             high=float('+inf'), \n",
    "                             shape=(1,), \n",
    "                             dtype=np.float32)\n",
    "        }\n",
    "        self.preprocessor = DictFlatteningPreprocessor(Dict(obs_spaces))\n",
    "        self.observation_space = self.preprocessor.observation_space\n",
    "        # Make the action space\n",
    "        self.action_space = Discrete(self.num_paths) # int between 0 and num_paths-1\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        # Create initial observations for each vehicle\n",
    "        start = {\n",
    "            'prev_route': 0,\n",
    "            'prev_time': 0\n",
    "        }\n",
    "        self.state = {'car_{}'.format(i): self.preprocessor.transform(start) for i in range(self.num_veh)}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        # Apply the actions of every agent at the same time\n",
    "        paths_flow_dict = {}\n",
    "        for agent, rl_action in action_dict.items():\n",
    "            # agent is one string that represent the id of the agent\n",
    "            # rl_action is one number that represent the path choice of the agent,\n",
    "            # rl_action should be a int between 0 and nb_paths-1\n",
    "            assert type(rl_action) == int and rl_action > -1 and rl_action < network.nb_paths\n",
    "            # we built a dictionnary paths_flow_dict that store the path flow on every path\n",
    "            if rl_action in paths_flow_dict:\n",
    "                paths_flow_dict[rl_action] += 1\n",
    "            else:\n",
    "                paths_flow_dict[rl_action] = 1\n",
    "\n",
    "        # update the path travel times of the network given the path flows\n",
    "        network.update_flow_from_dict(paths_flow_dict)\n",
    "        \n",
    "        \n",
    "        # Calculate states, reward, and done for each agent\n",
    "        travel_time = {}\n",
    "        marginal_cost = {}\n",
    "        \n",
    "        for agent, path_choice in action_dict.items():\n",
    "            assert type(path_choice) == int and path_choice > -1 and path_choice < network.nb_paths\n",
    "            # network travel time ( path ) return the travel time of the path\n",
    "            travel_time[agent] = network.travel_time(path_choice)\n",
    "            # network marginal cost ( path ) return the marginal cost of the path\n",
    "            marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "            new_obs = {\n",
    "                'prev_route': path_choice,\n",
    "                'prev_time': travel_time[agent]\n",
    "            }\n",
    "            obs_dict[agent] = self.preprocessor.transform(new_obs)\n",
    "            # Cost is the path_time\n",
    "            # rew_dict[agent] = reward_calculator(agent, marginal_cost)\n",
    "            # -path_choice # TO-DO: CHANGE THIS! \n",
    "            # Set done and infos\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "        rew_dict = reward_calculator(travel_time, marginal_cost, soc_fac)\n",
    "        self.state = obs_dict   \n",
    "        done[\"__all__\"] = True\n",
    "         \n",
    "        return obs_dict, rew_dict, done, info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code runs the experiment for the multiagent problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup policies for each vehicle\n",
    "\n",
    "network_name = 'Braess'\n",
    "nb_veh = 1\n",
    "# network init should build the Network object\n",
    "# ----- TO DO -----: import the network class\n",
    "network = network(network_name, nb_veh)\n",
    "# nb_path should be a property method\n",
    "\n",
    "\"\"\"\n",
    "define a function (class instantiation) which have for parameter a network name, \n",
    "and the number of vehicles, \n",
    "the return the num paths.\n",
    "\n",
    "interface of the class network\n",
    "class network:\n",
    "    def __init__(self, network_name, nb_veh):\n",
    "        load the network which correspond to the network_name\n",
    "        define the nb_veh as the nb_veh\n",
    "        from nb_veh and the intern demand define the number of flow that each veh represent\n",
    "        also define __nb_paths to give it to the Env\n",
    "\n",
    "    @property\n",
    "    def nb_paths(self):\n",
    "        return self.__nb_paths\n",
    "\"\"\"\n",
    "\n",
    "nb_path = network.nb_paths\n",
    "\n",
    "env_config = {\n",
    "    'network': network_name,\n",
    "    'num_veh': nb_veh,\n",
    "    'num_paths': nb_path,\n",
    "    'soc_fac': 0.5\n",
    "}\n",
    "routing_env = RoutingEnv(env_config)\n",
    "car_obs_space = routing_env.observation_space\n",
    "car_act_space = routing_env.action_space\n",
    "config = {\"gamma\": 0.85}\n",
    "policy_graphs = {\n",
    "    'vehicles': (DQNPolicyGraph, car_obs_space, car_act_space, config)\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: RoutingEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'DQN',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 10\n",
    "            },\n",
    "            'config': {\n",
    "                'env_config': env_config,\n",
    "                'multiagent': {\n",
    "                    'policy_graphs': policy_graphs,\n",
    "                    'policy_mapping_fn': tune.function(lambda agent_id: 'vehicles')\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Environment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
