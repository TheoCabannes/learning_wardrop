{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KS6wwFRc9CQK"
   },
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cslVfkBSLean"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TESTING ----\n",
      "The Nash case\n",
      "[3.75, 3.75, 3.75]\n",
      "[1.5, 0.75, 0.75]\n",
      "The social optimum case\n",
      "[3.25, 3.5, 3.5]\n",
      "[1.0, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import Networks\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_policy_graph import *\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import *\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.models.preprocessors import DictFlatteningPreprocessor, Preprocessor\n",
    "\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8YRoyVY687z"
   },
   "source": [
    "### Environment Below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73fz3Ogq6870"
   },
   "outputs": [],
   "source": [
    "def reward_calculator(travel_time, marginal_cost, soc_fac):\n",
    "    # ADD THE BREMIAN DIVERGENCE!!!\n",
    "    rew_dict = {}\n",
    "    for agent in travel_time.keys():\n",
    "        rew_dict[agent] = - (travel_time[agent] + soc_fac * marginal_cost[agent])\n",
    "    return rew_dict\n",
    "\n",
    "## Routing Environment \n",
    "class RoutingEnv(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        The cars start at the same origin point, Point A, and need to reach the same destination, Point Z. \n",
    "        Each car can reach Point Z via a variety of routing choices described in a given network, in which \n",
    "        each route introduces different travel times and congestion. \n",
    "        The goal is to minimize the average travel times amongst each of the cars.\n",
    "    \n",
    "    Observation: \n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tObservation                 Min          Max\n",
    "        0\tPrevious Route Choice        0      total_routes-1\n",
    "        1\tRoute Travel Time            0           +Inf\n",
    "        2\tComm Message               -Inf          +Inf\n",
    "        \n",
    "    Actions:\n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tAction                      Min          Max\n",
    "        0\tFuture Path_Choice           0      total_routes-1\n",
    "        1\tComm Message               -Inf          +Inf\n",
    "            \n",
    "    Reward:\n",
    "        Reward for each car is determined by the following formula: \n",
    "        marginal_cost = d[t(x_e)]/d[x_e]\n",
    "        Cost = route_travel_time + λ(marginal_cost)\n",
    "        Reward = -Cost\n",
    "        ***\n",
    "        route_travel_time: Travel time of the route previously taken by the car\n",
    "        marginal_cost: Cost that the car's route choice imposes on everyone else. \n",
    "                       The formula above captures the change in the travel flow \n",
    "                       with respect to the change in vehicle flow on a given road.\n",
    "        λ: Weight Toward Social Good (between 0 and 1)\n",
    "        \n",
    "    \n",
    "    Starting State:\n",
    "        All observations are assigned -1 for path choice and travel times.\n",
    "    \n",
    "    Episode Termination:\n",
    "        Cars keeps a consistent routing distribution.\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average travel time is less than or equal to the theorical social optimum. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        self.network_name = config['network']\n",
    "        self.num_paths = config['num_paths']\n",
    "        self.soc_fac = config['soc_fac']\n",
    "        self.num_veh = config['num_veh']\n",
    "        self.num_obs = 2\n",
    "        self.num_actions = 1\n",
    "        self.state = None\n",
    "        # Make observation space\n",
    "        obs_spaces = {\n",
    "            'prev_route': Discrete(self.num_paths),\n",
    "            'prev_time': Box(low=0, \n",
    "                             high=float('+inf'), \n",
    "                             shape=(1,), \n",
    "                             dtype=np.float32)\n",
    "        }\n",
    "        self.preprocessor = DictFlatteningPreprocessor(Dict(obs_spaces))\n",
    "        self.observation_space = self.preprocessor.observation_space\n",
    "        # Make the action space\n",
    "        self.action_space = Discrete(self.num_paths) # int between 0 and num_paths-1\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        # Create initial observations for each vehicle\n",
    "        start = {\n",
    "            'prev_route': 0,\n",
    "            'prev_time': 0\n",
    "        }\n",
    "        self.state = {'car_{}'.format(i): self.preprocessor.transform(start) for i in range(self.num_veh)}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        ### ADD THE COMMUNICATION CHANNEL\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        # Apply the actions of every agent at the same time\n",
    "        paths_flow_dict = {}\n",
    "        \n",
    "        for agent, rl_action in action_dict.items():\n",
    "            # agent is one string that represent the id of the agent\n",
    "            # rl_action is one number that represent the path choice of the agent,\n",
    "            # rl_action should be a int between 0 and nb_paths-1\n",
    "            rl_action = int(rl_action)\n",
    "            assert type(rl_action) == int and rl_action > -1 and rl_action < network.nb_paths\n",
    "            # we built a dictionnary paths_flow_dict that store the path flow on every path\n",
    "            if rl_action in paths_flow_dict:\n",
    "                paths_flow_dict[rl_action] += 1\n",
    "            else:\n",
    "                paths_flow_dict[rl_action] = 1\n",
    "\n",
    "        # update the path travel times of the network given the path flows\n",
    "        network.update_flow_from_dict(paths_flow_dict)\n",
    "        \n",
    "        \n",
    "        # Calculate states, reward, and done for each agent\n",
    "        travel_time = {}\n",
    "        marginal_cost = {}\n",
    "        \n",
    "        for agent, path_choice in action_dict.items():\n",
    "            path_choice = int(path_choice)\n",
    "            assert type(path_choice) == int and path_choice > -1 and path_choice < network.nb_paths\n",
    "            # network travel time ( path ) return the travel time of the path\n",
    "            travel_time[agent] = network.travel_time(path_choice)\n",
    "            # network marginal cost ( path ) return the marginal cost of the path\n",
    "            marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "            new_obs = {\n",
    "                'prev_route': path_choice,\n",
    "                'prev_time': travel_time[agent]\n",
    "            }\n",
    "            obs_dict[agent] = self.preprocessor.transform(new_obs)\n",
    "            # Cost is the path_time\n",
    "            # rew_dict[agent] = reward_calculator(agent, marginal_cost)\n",
    "            # -path_choice # TO-DO: CHANGE THIS! \n",
    "            # Set done and infos\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "        rew_dict = reward_calculator(travel_time, marginal_cost, self.soc_fac)\n",
    "        self.state = obs_dict   \n",
    "        \n",
    "        self.file = open(\"/Users/theophile/Documents/Classes/FLOW/Project/learning_wardrop/test_lambda_\" + str(self.soc_fac) + \"_gamma_\" + str(0) + \"_trail_\" + str(1), 'a')\n",
    "        self.file.write(\"Actions: \" + str(action_dict) + '\\n')\n",
    "        self.file.write(\"Reward: \" + str(rew_dict) + '\\n')\n",
    "        self.file.close()\n",
    "        \n",
    "        done[\"__all__\"] = True\n",
    "         \n",
    "        return obs_dict, rew_dict, done, info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code runs the experiment for the multiagent problem.\n",
    "\n",
    "Remark:\n",
    "On the Braess network using 4 vehicles, we should get:\n",
    "- if the social factor is 0, Nash: a reward of -3.75 in average, 2 cars on the first path, 1 on the second and third path\n",
    "- if the social factor is 1, Social optimum: a travel time of -3.5 in average (a reward of ), 2 cars on the first path, 1 on the second and third path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2018-12-02_14-17-27_4123/logs.\n",
      "Waiting for redis server at 127.0.0.1:46916 to respond...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for redis server at 127.0.0.1:15033 to respond...\n",
      "Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=e307b41ab410f6df019cf44bd7b3f7debfeccd4e5ee6fb2e\n",
      "======================================================================\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.9/17.2 GB\n",
      "\n",
      "Created LogSyncer for /Users/theophile/ray_results/route-DQN/DQN_multi_routing_0_2018-12-02_14-17-28058wpch7 -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 8.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-17-42\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.0235\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 1000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 10.741\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.755875\n",
      "  time_since_restore: 3.111064910888672\n",
      "  time_this_iter_s: 3.111064910888672\n",
      "  time_total_s: 3.111064910888672\n",
      "  timestamp: 1543789062\n",
      "  timesteps_since_restore: 1000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 3 s, 1 iter, 1000 ts, -15 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-17-48\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.008\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 2000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.087\n",
      "    max_exploration: 0.902\n",
      "    min_exploration: 0.902\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8000\n",
      "    num_target_updates: 3\n",
      "    opt_peak_throughput: 4515.618\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.137\n",
      "    sample_time_ms: 11.817\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.752\n",
      "  time_since_restore: 9.069880962371826\n",
      "  time_this_iter_s: 5.958816051483154\n",
      "  time_total_s: 9.069880962371826\n",
      "  timestamp: 1543789068\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 9 s, 2 iter, 2000 ts, -15 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.0655\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 3000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.488\n",
      "    max_exploration: 0.804\n",
      "    min_exploration: 0.804\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16000\n",
      "    num_target_updates: 5\n",
      "    opt_peak_throughput: 4273.674\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.412\n",
      "    sample_time_ms: 12.425\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.766375\n",
      "  time_since_restore: 14.503100156784058\n",
      "  time_this_iter_s: 5.4332191944122314\n",
      "  time_total_s: 14.503100156784058\n",
      "  timestamp: 1543789074\n",
      "  timesteps_since_restore: 3000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 14 s, 3 iter, 3000 ts, -15.1 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-17-59\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.138\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 4000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.918\n",
      "    max_exploration: 0.706\n",
      "    min_exploration: 0.706\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 24000\n",
      "    num_target_updates: 7\n",
      "    opt_peak_throughput: 4625.948\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.12\n",
      "    sample_time_ms: 11.215\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.7845\n",
      "  time_since_restore: 19.925739288330078\n",
      "  time_this_iter_s: 5.4226391315460205\n",
      "  time_total_s: 19.925739288330078\n",
      "  timestamp: 1543789079\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 19 s, 4 iter, 4000 ts, -15.1 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-05\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.211\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 5000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.257\n",
      "    max_exploration: 0.608\n",
      "    min_exploration: 0.608\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32000\n",
      "    num_target_updates: 9\n",
      "    opt_peak_throughput: 4409.53\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.2\n",
      "    sample_time_ms: 11.129\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.80275\n",
      "  time_since_restore: 25.525859117507935\n",
      "  time_this_iter_s: 5.6001198291778564\n",
      "  time_total_s: 25.525859117507935\n",
      "  timestamp: 1543789085\n",
      "  timesteps_since_restore: 5000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 25 s, 5 iter, 5000 ts, -15.2 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.223\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 6000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.007\n",
      "    max_exploration: 0.51\n",
      "    min_exploration: 0.51\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 40000\n",
      "    num_target_updates: 11\n",
      "    opt_peak_throughput: 4566.611\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.184\n",
      "    sample_time_ms: 11.109\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.80575\n",
      "  time_since_restore: 31.052665948867798\n",
      "  time_this_iter_s: 5.526806831359863\n",
      "  time_total_s: 31.052665948867798\n",
      "  timestamp: 1543789090\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 31 s, 6 iter, 6000 ts, -15.2 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-16\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.3655\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 7000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.733\n",
      "    max_exploration: 0.41200000000000003\n",
      "    min_exploration: 0.41200000000000003\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 48000\n",
      "    num_target_updates: 13\n",
      "    opt_peak_throughput: 4138.001\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.418\n",
      "    sample_time_ms: 12.639\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.841375\n",
      "  time_since_restore: 36.64009618759155\n",
      "  time_this_iter_s: 5.587430238723755\n",
      "  time_total_s: 36.64009618759155\n",
      "  timestamp: 1543789096\n",
      "  timesteps_since_restore: 7000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 36 s, 7 iter, 7000 ts, -15.4 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-22\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.537\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 8000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.15\n",
      "    max_exploration: 0.31400000000000006\n",
      "    min_exploration: 0.31400000000000006\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 56000\n",
      "    num_target_updates: 15\n",
      "    opt_peak_throughput: 4475.297\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.322\n",
      "    sample_time_ms: 11.562\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.88425\n",
      "  time_since_restore: 42.39857220649719\n",
      "  time_this_iter_s: 5.75847601890564\n",
      "  time_total_s: 42.39857220649719\n",
      "  timestamp: 1543789102\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 42 s, 8 iter, 8000 ts, -15.5 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.719\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 9000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.176\n",
      "    max_exploration: 0.21599999999999997\n",
      "    min_exploration: 0.21599999999999997\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 64000\n",
      "    num_target_updates: 17\n",
      "    opt_peak_throughput: 4459.401\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.2\n",
      "    sample_time_ms: 11.72\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.92975\n",
      "  time_since_restore: 48.27733302116394\n",
      "  time_this_iter_s: 5.878760814666748\n",
      "  time_total_s: 48.27733302116394\n",
      "  timestamp: 1543789108\n",
      "  timesteps_since_restore: 9000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 48 s, 9 iter, 9000 ts, -15.7 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.8395\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 10000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.267\n",
      "    max_exploration: 0.118\n",
      "    min_exploration: 0.118\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 72000\n",
      "    num_target_updates: 19\n",
      "    opt_peak_throughput: 4403.368\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.348\n",
      "    sample_time_ms: 11.811\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.959875\n",
      "  time_since_restore: 54.08042073249817\n",
      "  time_this_iter_s: 5.8030877113342285\n",
      "  time_total_s: 54.08042073249817\n",
      "  timestamp: 1543789114\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 54 s, 10 iter, 10000 ts, -15.8 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-39\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.118\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 11000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.463\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 80000\n",
      "    num_target_updates: 21\n",
      "    opt_peak_throughput: 4287.668\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.464\n",
      "    sample_time_ms: 12.504\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.0295\n",
      "  time_since_restore: 59.83473181724548\n",
      "  time_this_iter_s: 5.7543110847473145\n",
      "  time_total_s: 59.83473181724548\n",
      "  timestamp: 1543789119\n",
      "  timesteps_since_restore: 11000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 59 s, 11 iter, 11000 ts, -16.1 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-45\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.048\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 12000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.19\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 88000\n",
      "    num_target_updates: 23\n",
      "    opt_peak_throughput: 4450.721\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.276\n",
      "    sample_time_ms: 12.026\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.012\n",
      "  time_since_restore: 65.51770687103271\n",
      "  time_this_iter_s: 5.6829750537872314\n",
      "  time_total_s: 65.51770687103271\n",
      "  timestamp: 1543789125\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 65 s, 12 iter, 12000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-50\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.1365\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 13000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.934\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 96000\n",
      "    num_target_updates: 25\n",
      "    opt_peak_throughput: 4614.862\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.186\n",
      "    sample_time_ms: 11.309\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.034125\n",
      "  time_since_restore: 70.99244785308838\n",
      "  time_this_iter_s: 5.474740982055664\n",
      "  time_total_s: 70.99244785308838\n",
      "  timestamp: 1543789130\n",
      "  timesteps_since_restore: 13000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 70 s, 13 iter, 13000 ts, -16.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0205\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 14000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.165\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 104000\n",
      "    num_target_updates: 27\n",
      "    opt_peak_throughput: 4465.96\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.063\n",
      "    sample_time_ms: 11.439\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.005125\n",
      "  time_since_restore: 76.59287691116333\n",
      "  time_this_iter_s: 5.600429058074951\n",
      "  time_total_s: 76.59287691116333\n",
      "  timestamp: 1543789136\n",
      "  timesteps_since_restore: 14000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 76 s, 14 iter, 14000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-02\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.102\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 15000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.194\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 112000\n",
      "    num_target_updates: 29\n",
      "    opt_peak_throughput: 4448.214\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.34\n",
      "    sample_time_ms: 12.348\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.0255\n",
      "  time_since_restore: 82.38369488716125\n",
      "  time_this_iter_s: 5.790817975997925\n",
      "  time_total_s: 82.38369488716125\n",
      "  timestamp: 1543789142\n",
      "  timesteps_since_restore: 15000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 82 s, 15 iter, 15000 ts, -16.1 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-08\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0735\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 16000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.159\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 120000\n",
      "    num_target_updates: 31\n",
      "    opt_peak_throughput: 4470.14\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.24\n",
      "    sample_time_ms: 12.124\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.018375\n",
      "  time_since_restore: 88.1863067150116\n",
      "  time_this_iter_s: 5.802611827850342\n",
      "  time_total_s: 88.1863067150116\n",
      "  timestamp: 1543789148\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 88 s, 16 iter, 16000 ts, -16.1 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-13\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -16.0155\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 17000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.21\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 128000\n",
      "    num_target_updates: 33\n",
      "    opt_peak_throughput: 4438.109\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.194\n",
      "    sample_time_ms: 11.226\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.003875\n",
      "  time_since_restore: 93.6564633846283\n",
      "  time_this_iter_s: 5.470156669616699\n",
      "  time_total_s: 93.6564633846283\n",
      "  timestamp: 1543789153\n",
      "  timesteps_since_restore: 17000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 93 s, 17 iter, 17000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-19\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9485\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 18000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.237\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 136000\n",
      "    num_target_updates: 35\n",
      "    opt_peak_throughput: 4422.024\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.395\n",
      "    sample_time_ms: 12.291\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987125\n",
      "  time_since_restore: 99.41257834434509\n",
      "  time_this_iter_s: 5.756114959716797\n",
      "  time_total_s: 99.41257834434509\n",
      "  timestamp: 1543789159\n",
      "  timesteps_since_restore: 18000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 99 s, 18 iter, 18000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-24\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.969\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 19000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.261\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 144000\n",
      "    num_target_updates: 37\n",
      "    opt_peak_throughput: 4406.997\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.219\n",
      "    sample_time_ms: 11.866\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.99225\n",
      "  time_since_restore: 104.91491627693176\n",
      "  time_this_iter_s: 5.50233793258667\n",
      "  time_total_s: 104.91491627693176\n",
      "  timestamp: 1543789164\n",
      "  timesteps_since_restore: 19000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 104 s, 19 iter, 19000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-30\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9435\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 20000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.619\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 152000\n",
      "    num_target_updates: 39\n",
      "    opt_peak_throughput: 4199.948\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.491\n",
      "    sample_time_ms: 12.185\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.985875\n",
      "  time_since_restore: 110.78363037109375\n",
      "  time_this_iter_s: 5.868714094161987\n",
      "  time_total_s: 110.78363037109375\n",
      "  timestamp: 1543789170\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 110 s, 20 iter, 20000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-36\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9765\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 21000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.908\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 160000\n",
      "    num_target_updates: 41\n",
      "    opt_peak_throughput: 4632.319\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.157\n",
      "    sample_time_ms: 11.234\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.994125\n",
      "  time_since_restore: 116.47929525375366\n",
      "  time_this_iter_s: 5.695664882659912\n",
      "  time_total_s: 116.47929525375366\n",
      "  timestamp: 1543789176\n",
      "  timesteps_since_restore: 21000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 116 s, 21 iter, 21000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-42\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9515\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 22000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.011\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 168000\n",
      "    num_target_updates: 43\n",
      "    opt_peak_throughput: 4564.546\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.21\n",
      "    sample_time_ms: 11.669\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987875\n",
      "  time_since_restore: 122.15448021888733\n",
      "  time_this_iter_s: 5.675184965133667\n",
      "  time_total_s: 122.15448021888733\n",
      "  timestamp: 1543789182\n",
      "  timesteps_since_restore: 22000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 122 s, 22 iter, 22000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-47\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9425\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 23000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.485\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 176000\n",
      "    num_target_updates: 45\n",
      "    opt_peak_throughput: 4274.981\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.537\n",
      "    sample_time_ms: 12.965\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.985625\n",
      "  time_since_restore: 127.85578417778015\n",
      "  time_this_iter_s: 5.701303958892822\n",
      "  time_total_s: 127.85578417778015\n",
      "  timestamp: 1543789187\n",
      "  timesteps_since_restore: 23000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 127 s, 23 iter, 23000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-53\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9565\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 24000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.969\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 184000\n",
      "    num_target_updates: 47\n",
      "    opt_peak_throughput: 4591.937\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.258\n",
      "    sample_time_ms: 11.94\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.989125\n",
      "  time_since_restore: 133.62468028068542\n",
      "  time_this_iter_s: 5.768896102905273\n",
      "  time_total_s: 133.62468028068542\n",
      "  timestamp: 1543789193\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 133 s, 24 iter, 24000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-19-59\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9475\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 25000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.74\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 192000\n",
      "    num_target_updates: 49\n",
      "    opt_peak_throughput: 4747.776\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.216\n",
      "    sample_time_ms: 11.665\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.986875\n",
      "  time_since_restore: 139.38138222694397\n",
      "  time_this_iter_s: 5.756701946258545\n",
      "  time_total_s: 139.38138222694397\n",
      "  timestamp: 1543789199\n",
      "  timesteps_since_restore: 25000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 139 s, 25 iter, 25000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-05\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.017\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 26000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.769\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 200000\n",
      "    num_target_updates: 51\n",
      "    opt_peak_throughput: 4727.441\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.206\n",
      "    sample_time_ms: 11.87\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.00425\n",
      "  time_since_restore: 144.85292315483093\n",
      "  time_this_iter_s: 5.471540927886963\n",
      "  time_total_s: 144.85292315483093\n",
      "  timestamp: 1543789205\n",
      "  timesteps_since_restore: 26000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 144 s, 26 iter, 26000 ts, -16 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-10\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.979\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 27000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.051\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 208000\n",
      "    num_target_updates: 53\n",
      "    opt_peak_throughput: 4538.354\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.311\n",
      "    sample_time_ms: 11.609\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.99475\n",
      "  time_since_restore: 150.59256219863892\n",
      "  time_this_iter_s: 5.739639043807983\n",
      "  time_total_s: 150.59256219863892\n",
      "  timestamp: 1543789210\n",
      "  timesteps_since_restore: 27000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 150 s, 27 iter, 27000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-16\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0205\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 28000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.453\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 216000\n",
      "    num_target_updates: 55\n",
      "    opt_peak_throughput: 4958.649\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.284\n",
      "    sample_time_ms: 11.256\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.005125\n",
      "  time_since_restore: 156.13057041168213\n",
      "  time_this_iter_s: 5.538008213043213\n",
      "  time_total_s: 156.13057041168213\n",
      "  timestamp: 1543789216\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 156 s, 28 iter, 28000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-22\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0225\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 29000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.715\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 224000\n",
      "    num_target_updates: 57\n",
      "    opt_peak_throughput: 4765.459\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.13\n",
      "    sample_time_ms: 11.449\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.005625\n",
      "  time_since_restore: 161.77218055725098\n",
      "  time_this_iter_s: 5.641610145568848\n",
      "  time_total_s: 161.77218055725098\n",
      "  timestamp: 1543789222\n",
      "  timesteps_since_restore: 29000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 161 s, 29 iter, 29000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-27\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -16.0025\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 30000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.692\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 232000\n",
      "    num_target_updates: 59\n",
      "    opt_peak_throughput: 4782.099\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.148\n",
      "    sample_time_ms: 11.784\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.000625\n",
      "  time_since_restore: 167.25805163383484\n",
      "  time_this_iter_s: 5.485871076583862\n",
      "  time_total_s: 167.25805163383484\n",
      "  timestamp: 1543789227\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 167 s, 30 iter, 30000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-33\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.9885\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 31000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.342\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 240000\n",
      "    num_target_updates: 61\n",
      "    opt_peak_throughput: 4358.539\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.421\n",
      "    sample_time_ms: 12.023\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.997125\n",
      "  time_since_restore: 172.84984946250916\n",
      "  time_this_iter_s: 5.591797828674316\n",
      "  time_total_s: 172.84984946250916\n",
      "  timestamp: 1543789233\n",
      "  timesteps_since_restore: 31000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 172 s, 31 iter, 31000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-38\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.957\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 32000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.675\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 248000\n",
      "    num_target_updates: 63\n",
      "    opt_peak_throughput: 4793.884\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.303\n",
      "    sample_time_ms: 12.024\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98925\n",
      "  time_since_restore: 178.54063272476196\n",
      "  time_this_iter_s: 5.690783262252808\n",
      "  time_total_s: 178.54063272476196\n",
      "  timestamp: 1543789238\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 178 s, 32 iter, 32000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-44\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.96\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 33000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.031\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 256000\n",
      "    num_target_updates: 65\n",
      "    opt_peak_throughput: 4551.065\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.237\n",
      "    sample_time_ms: 12.123\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.99\n",
      "  time_since_restore: 184.10508346557617\n",
      "  time_this_iter_s: 5.564450740814209\n",
      "  time_total_s: 184.10508346557617\n",
      "  timestamp: 1543789244\n",
      "  timesteps_since_restore: 33000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 184 s, 33 iter, 33000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-50\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -16.029\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 34000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.835\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 264000\n",
      "    num_target_updates: 67\n",
      "    opt_peak_throughput: 4681.584\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.207\n",
      "    sample_time_ms: 11.974\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.00725\n",
      "  time_since_restore: 189.87636637687683\n",
      "  time_this_iter_s: 5.771282911300659\n",
      "  time_total_s: 189.87636637687683\n",
      "  timestamp: 1543789250\n",
      "  timesteps_since_restore: 34000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 189 s, 34 iter, 34000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-20-55\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.954\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 35000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.112\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 272000\n",
      "    num_target_updates: 69\n",
      "    opt_peak_throughput: 4499.33\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.387\n",
      "    sample_time_ms: 12.127\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.9885\n",
      "  time_since_restore: 195.41286420822144\n",
      "  time_this_iter_s: 5.5364978313446045\n",
      "  time_total_s: 195.41286420822144\n",
      "  timestamp: 1543789255\n",
      "  timesteps_since_restore: 35000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 195 s, 35 iter, 35000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9415\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 36000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.357\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 280000\n",
      "    num_target_updates: 71\n",
      "    opt_peak_throughput: 4349.471\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.264\n",
      "    sample_time_ms: 11.884\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.985375\n",
      "  time_since_restore: 201.19493913650513\n",
      "  time_this_iter_s: 5.782074928283691\n",
      "  time_total_s: 201.19493913650513\n",
      "  timestamp: 1543789261\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 201 s, 36 iter, 36000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-07\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9575\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 37000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.81\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 288000\n",
      "    num_target_updates: 73\n",
      "    opt_peak_throughput: 4698.645\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.119\n",
      "    sample_time_ms: 11.114\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.989375\n",
      "  time_since_restore: 206.7739062309265\n",
      "  time_this_iter_s: 5.578967094421387\n",
      "  time_total_s: 206.7739062309265\n",
      "  timestamp: 1543789267\n",
      "  timesteps_since_restore: 37000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 206 s, 37 iter, 37000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-12\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.956\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 38000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.242\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 296000\n",
      "    num_target_updates: 75\n",
      "    opt_peak_throughput: 4418.691\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.498\n",
      "    sample_time_ms: 12.392\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.989\n",
      "  time_since_restore: 212.29746532440186\n",
      "  time_this_iter_s: 5.523559093475342\n",
      "  time_total_s: 212.29746532440186\n",
      "  timestamp: 1543789272\n",
      "  timesteps_since_restore: 38000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 212 s, 38 iter, 38000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-18\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.946\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 39000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.862\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 304000\n",
      "    num_target_updates: 77\n",
      "    opt_peak_throughput: 4663.171\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.252\n",
      "    sample_time_ms: 11.965\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.9865\n",
      "  time_since_restore: 218.00197315216064\n",
      "  time_this_iter_s: 5.704507827758789\n",
      "  time_total_s: 218.00197315216064\n",
      "  timestamp: 1543789278\n",
      "  timesteps_since_restore: 39000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 218 s, 39 iter, 39000 ts, -15.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-24\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9505\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 40000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.988\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 312000\n",
      "    num_target_updates: 79\n",
      "    opt_peak_throughput: 4579.247\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.08\n",
      "    sample_time_ms: 11.013\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987625\n",
      "  time_since_restore: 223.81462216377258\n",
      "  time_this_iter_s: 5.8126490116119385\n",
      "  time_total_s: 223.81462216377258\n",
      "  timestamp: 1543789284\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 223 s, 40 iter, 40000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.951\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 41000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.826\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 320000\n",
      "    num_target_updates: 81\n",
      "    opt_peak_throughput: 4687.879\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.154\n",
      "    sample_time_ms: 11.747\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98775\n",
      "  time_since_restore: 229.17714500427246\n",
      "  time_this_iter_s: 5.362522840499878\n",
      "  time_total_s: 229.17714500427246\n",
      "  timestamp: 1543789289\n",
      "  timesteps_since_restore: 41000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 229 s, 41 iter, 41000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-35\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.047\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 42000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.793\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 328000\n",
      "    num_target_updates: 83\n",
      "    opt_peak_throughput: 4710.766\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.273\n",
      "    sample_time_ms: 11.092\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.01175\n",
      "  time_since_restore: 234.56972098350525\n",
      "  time_this_iter_s: 5.392575979232788\n",
      "  time_total_s: 234.56972098350525\n",
      "  timestamp: 1543789295\n",
      "  timesteps_since_restore: 42000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 234 s, 42 iter, 42000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-40\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.948\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 43000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.896\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 336000\n",
      "    num_target_updates: 85\n",
      "    opt_peak_throughput: 4640.663\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.289\n",
      "    sample_time_ms: 11.501\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987\n",
      "  time_since_restore: 240.0300362110138\n",
      "  time_this_iter_s: 5.460315227508545\n",
      "  time_total_s: 240.0300362110138\n",
      "  timestamp: 1543789300\n",
      "  timesteps_since_restore: 43000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 240 s, 43 iter, 43000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-46\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.036\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 44000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.868\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 344000\n",
      "    num_target_updates: 87\n",
      "    opt_peak_throughput: 4659.367\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.22\n",
      "    sample_time_ms: 11.55\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.009\n",
      "  time_since_restore: 245.57854413986206\n",
      "  time_this_iter_s: 5.548507928848267\n",
      "  time_total_s: 245.57854413986206\n",
      "  timestamp: 1543789306\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 245 s, 44 iter, 44000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-51\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.943\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 45000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.021\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 352000\n",
      "    num_target_updates: 89\n",
      "    opt_peak_throughput: 4557.463\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.231\n",
      "    sample_time_ms: 11.405\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98575\n",
      "  time_since_restore: 251.0066339969635\n",
      "  time_this_iter_s: 5.42808985710144\n",
      "  time_total_s: 251.0066339969635\n",
      "  timestamp: 1543789311\n",
      "  timesteps_since_restore: 45000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 251 s, 45 iter, 45000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-21-57\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0265\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 46000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.188\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 360000\n",
      "    num_target_updates: 91\n",
      "    opt_peak_throughput: 4451.976\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.309\n",
      "    sample_time_ms: 12.102\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.006625\n",
      "  time_since_restore: 256.53539991378784\n",
      "  time_this_iter_s: 5.528765916824341\n",
      "  time_total_s: 256.53539991378784\n",
      "  timestamp: 1543789317\n",
      "  timesteps_since_restore: 46000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 256 s, 46 iter, 46000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-02\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9395\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 47000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.664\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 368000\n",
      "    num_target_updates: 93\n",
      "    opt_peak_throughput: 4175.514\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.537\n",
      "    sample_time_ms: 13.268\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.984875\n",
      "  time_since_restore: 262.2825200557709\n",
      "  time_this_iter_s: 5.747120141983032\n",
      "  time_total_s: 262.2825200557709\n",
      "  timestamp: 1543789322\n",
      "  timesteps_since_restore: 47000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 262 s, 47 iter, 47000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.953\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 48000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.38\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 376000\n",
      "    num_target_updates: 95\n",
      "    opt_peak_throughput: 4336.262\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.351\n",
      "    sample_time_ms: 12.579\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98825\n",
      "  time_since_restore: 267.9625859260559\n",
      "  time_this_iter_s: 5.680065870285034\n",
      "  time_total_s: 267.9625859260559\n",
      "  timestamp: 1543789328\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 267 s, 48 iter, 48000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-14\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.944\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 49000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.522\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 384000\n",
      "    num_target_updates: 97\n",
      "    opt_peak_throughput: 4254.195\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.354\n",
      "    sample_time_ms: 13.174\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.986\n",
      "  time_since_restore: 273.66260981559753\n",
      "  time_this_iter_s: 5.700023889541626\n",
      "  time_total_s: 273.66260981559753\n",
      "  timestamp: 1543789334\n",
      "  timesteps_since_restore: 49000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 273 s, 49 iter, 49000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-19\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.934\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 50000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.31\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 392000\n",
      "    num_target_updates: 99\n",
      "    opt_peak_throughput: 4377.645\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.379\n",
      "    sample_time_ms: 12.313\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.9835\n",
      "  time_since_restore: 279.2986578941345\n",
      "  time_this_iter_s: 5.636048078536987\n",
      "  time_total_s: 279.2986578941345\n",
      "  timestamp: 1543789339\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 279 s, 50 iter, 50000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-25\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9535\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 51000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.737\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 400000\n",
      "    num_target_updates: 101\n",
      "    opt_peak_throughput: 4750.028\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.173\n",
      "    sample_time_ms: 11.297\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.988375\n",
      "  time_since_restore: 284.8379096984863\n",
      "  time_this_iter_s: 5.539251804351807\n",
      "  time_total_s: 284.8379096984863\n",
      "  timestamp: 1543789345\n",
      "  timesteps_since_restore: 51000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 284 s, 51 iter, 51000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-30\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9765\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 52000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.211\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 408000\n",
      "    num_target_updates: 103\n",
      "    opt_peak_throughput: 4437.669\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.024\n",
      "    sample_time_ms: 10.855\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.994125\n",
      "  time_since_restore: 290.1059546470642\n",
      "  time_this_iter_s: 5.268044948577881\n",
      "  time_total_s: 290.1059546470642\n",
      "  timestamp: 1543789350\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 290 s, 52 iter, 52000 ts, -16 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.975\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 53000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.505\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 416000\n",
      "    num_target_updates: 105\n",
      "    opt_peak_throughput: 4264.102\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.593\n",
      "    sample_time_ms: 13.009\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.99375\n",
      "  time_since_restore: 295.90749430656433\n",
      "  time_this_iter_s: 5.801539659500122\n",
      "  time_total_s: 295.90749430656433\n",
      "  timestamp: 1543789356\n",
      "  timesteps_since_restore: 53000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 295 s, 53 iter, 53000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-42\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.948\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 54000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.671\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 424000\n",
      "    num_target_updates: 107\n",
      "    opt_peak_throughput: 4796.848\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.202\n",
      "    sample_time_ms: 11.546\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987\n",
      "  time_since_restore: 301.45944142341614\n",
      "  time_this_iter_s: 5.551947116851807\n",
      "  time_total_s: 301.45944142341614\n",
      "  timestamp: 1543789362\n",
      "  timesteps_since_restore: 54000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 301 s, 54 iter, 54000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-47\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.971\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 55000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.982\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 432000\n",
      "    num_target_updates: 109\n",
      "    opt_peak_throughput: 4582.921\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.252\n",
      "    sample_time_ms: 11.277\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.99275\n",
      "  time_since_restore: 306.8954553604126\n",
      "  time_this_iter_s: 5.43601393699646\n",
      "  time_total_s: 306.8954553604126\n",
      "  timestamp: 1543789367\n",
      "  timesteps_since_restore: 55000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 306 s, 55 iter, 55000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.987\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 56000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.199\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 440000\n",
      "    num_target_updates: 111\n",
      "    opt_peak_throughput: 4444.943\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.403\n",
      "    sample_time_ms: 12.484\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.99675\n",
      "  time_since_restore: 312.69735765457153\n",
      "  time_this_iter_s: 5.8019022941589355\n",
      "  time_total_s: 312.69735765457153\n",
      "  timestamp: 1543789373\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 312 s, 56 iter, 56000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-22-59\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9485\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 57000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.663\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 448000\n",
      "    num_target_updates: 113\n",
      "    opt_peak_throughput: 4802.289\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.246\n",
      "    sample_time_ms: 11.573\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987125\n",
      "  time_since_restore: 318.3834125995636\n",
      "  time_this_iter_s: 5.686054944992065\n",
      "  time_total_s: 318.3834125995636\n",
      "  timestamp: 1543789379\n",
      "  timesteps_since_restore: 57000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 318 s, 57 iter, 57000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-04\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.947\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 58000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.524\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 456000\n",
      "    num_target_updates: 115\n",
      "    opt_peak_throughput: 4252.888\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.712\n",
      "    sample_time_ms: 12.58\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98675\n",
      "  time_since_restore: 324.26638555526733\n",
      "  time_this_iter_s: 5.882972955703735\n",
      "  time_total_s: 324.26638555526733\n",
      "  timestamp: 1543789384\n",
      "  timesteps_since_restore: 58000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 324 s, 58 iter, 58000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-10\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0685\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 59000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.021\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 464000\n",
      "    num_target_updates: 117\n",
      "    opt_peak_throughput: 4558.035\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.109\n",
      "    sample_time_ms: 11.979\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.017125\n",
      "  time_since_restore: 329.803763628006\n",
      "  time_this_iter_s: 5.5373780727386475\n",
      "  time_total_s: 329.803763628006\n",
      "  timestamp: 1543789390\n",
      "  timesteps_since_restore: 59000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 329 s, 59 iter, 59000 ts, -16.1 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-16\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.944\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 60000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.638\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 472000\n",
      "    num_target_updates: 119\n",
      "    opt_peak_throughput: 4821.056\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.257\n",
      "    sample_time_ms: 11.405\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.986\n",
      "  time_since_restore: 335.3806085586548\n",
      "  time_this_iter_s: 5.576844930648804\n",
      "  time_total_s: 335.3806085586548\n",
      "  timestamp: 1543789396\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 335 s, 60 iter, 60000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-21\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9505\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 61000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.423\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 480000\n",
      "    num_target_updates: 121\n",
      "    opt_peak_throughput: 4310.97\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.383\n",
      "    sample_time_ms: 12.435\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987625\n",
      "  time_since_restore: 341.23942947387695\n",
      "  time_this_iter_s: 5.858820915222168\n",
      "  time_total_s: 341.23942947387695\n",
      "  timestamp: 1543789401\n",
      "  timesteps_since_restore: 61000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 341 s, 61 iter, 61000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-27\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.006\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 62000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.011\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 488000\n",
      "    num_target_updates: 123\n",
      "    opt_peak_throughput: 4564.034\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.553\n",
      "    sample_time_ms: 11.947\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.0015\n",
      "  time_since_restore: 346.79851150512695\n",
      "  time_this_iter_s: 5.55908203125\n",
      "  time_total_s: 346.79851150512695\n",
      "  timestamp: 1543789407\n",
      "  timesteps_since_restore: 62000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 346 s, 62 iter, 62000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-33\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0065\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 63000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.214\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 496000\n",
      "    num_target_updates: 125\n",
      "    opt_peak_throughput: 4435.733\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.519\n",
      "    sample_time_ms: 12.577\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.001625\n",
      "  time_since_restore: 352.61289739608765\n",
      "  time_this_iter_s: 5.814385890960693\n",
      "  time_total_s: 352.61289739608765\n",
      "  timestamp: 1543789413\n",
      "  timesteps_since_restore: 63000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 352 s, 63 iter, 63000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-38\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.964\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 64000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.897\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 504000\n",
      "    num_target_updates: 126\n",
      "    opt_peak_throughput: 4639.508\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.324\n",
      "    sample_time_ms: 11.896\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.991\n",
      "  time_since_restore: 358.1466383934021\n",
      "  time_this_iter_s: 5.533740997314453\n",
      "  time_total_s: 358.1466383934021\n",
      "  timestamp: 1543789418\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.9/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 358 s, 64 iter, 64000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-44\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.949\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 65000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.938\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 512000\n",
      "    num_target_updates: 128\n",
      "    opt_peak_throughput: 4612.499\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.257\n",
      "    sample_time_ms: 11.899\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98725\n",
      "  time_since_restore: 363.6864547729492\n",
      "  time_this_iter_s: 5.539816379547119\n",
      "  time_total_s: 363.6864547729492\n",
      "  timestamp: 1543789424\n",
      "  timesteps_since_restore: 65000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 363 s, 65 iter, 65000 ts, -15.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-50\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.952\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 66000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.785\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 520000\n",
      "    num_target_updates: 130\n",
      "    opt_peak_throughput: 4716.527\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.122\n",
      "    sample_time_ms: 11.269\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.988\n",
      "  time_since_restore: 369.21275067329407\n",
      "  time_this_iter_s: 5.526295900344849\n",
      "  time_total_s: 369.21275067329407\n",
      "  timestamp: 1543789430\n",
      "  timesteps_since_restore: 66000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 369 s, 66 iter, 66000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-23-55\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9505\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 67000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.213\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 528000\n",
      "    num_target_updates: 132\n",
      "    opt_peak_throughput: 4436.143\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.498\n",
      "    sample_time_ms: 12.215\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987625\n",
      "  time_since_restore: 374.73756980895996\n",
      "  time_this_iter_s: 5.5248191356658936\n",
      "  time_total_s: 374.73756980895996\n",
      "  timestamp: 1543789435\n",
      "  timesteps_since_restore: 67000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 374 s, 67 iter, 67000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-01\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.941\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 68000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.493\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 536000\n",
      "    num_target_updates: 134\n",
      "    opt_peak_throughput: 4270.655\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.515\n",
      "    sample_time_ms: 13.661\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98525\n",
      "  time_since_restore: 380.3384349346161\n",
      "  time_this_iter_s: 5.600865125656128\n",
      "  time_total_s: 380.3384349346161\n",
      "  timestamp: 1543789441\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 380 s, 68 iter, 68000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-06\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.9355\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 69000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.961\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 544000\n",
      "    num_target_updates: 136\n",
      "    opt_peak_throughput: 4597.033\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.179\n",
      "    sample_time_ms: 11.14\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.983875\n",
      "  time_since_restore: 385.8291208744049\n",
      "  time_this_iter_s: 5.490685939788818\n",
      "  time_total_s: 385.8291208744049\n",
      "  timestamp: 1543789446\n",
      "  timesteps_since_restore: 69000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 385 s, 69 iter, 69000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-12\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.9515\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 70000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.066\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 552000\n",
      "    num_target_updates: 138\n",
      "    opt_peak_throughput: 4528.676\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.211\n",
      "    sample_time_ms: 11.369\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.987875\n",
      "  time_since_restore: 391.28155517578125\n",
      "  time_this_iter_s: 5.452434301376343\n",
      "  time_total_s: 391.28155517578125\n",
      "  timestamp: 1543789452\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 391 s, 70 iter, 70000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-17\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.947\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 71000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.086\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 560000\n",
      "    num_target_updates: 140\n",
      "    opt_peak_throughput: 4516.029\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.068\n",
      "    sample_time_ms: 10.992\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98675\n",
      "  time_since_restore: 396.6431005001068\n",
      "  time_this_iter_s: 5.3615453243255615\n",
      "  time_total_s: 396.6431005001068\n",
      "  timestamp: 1543789457\n",
      "  timesteps_since_restore: 71000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 396 s, 71 iter, 71000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-22\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.962\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 72000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.123\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 568000\n",
      "    num_target_updates: 142\n",
      "    opt_peak_throughput: 4492.463\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.226\n",
      "    sample_time_ms: 12.065\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.9905\n",
      "  time_since_restore: 402.0847225189209\n",
      "  time_this_iter_s: 5.441622018814087\n",
      "  time_total_s: 402.0847225189209\n",
      "  timestamp: 1543789462\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 402 s, 72 iter, 72000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0075\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 73000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.071\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 576000\n",
      "    num_target_updates: 144\n",
      "    opt_peak_throughput: 4525.424\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.167\n",
      "    sample_time_ms: 11.058\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.001875\n",
      "  time_since_restore: 407.5464942455292\n",
      "  time_this_iter_s: 5.461771726608276\n",
      "  time_total_s: 407.5464942455292\n",
      "  timestamp: 1543789468\n",
      "  timesteps_since_restore: 73000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 407 s, 73 iter, 73000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-33\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.947\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 74000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.77\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 584000\n",
      "    num_target_updates: 146\n",
      "    opt_peak_throughput: 4118.207\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.774\n",
      "    sample_time_ms: 13.926\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.98675\n",
      "  time_since_restore: 413.04425406455994\n",
      "  time_this_iter_s: 5.497759819030762\n",
      "  time_total_s: 413.04425406455994\n",
      "  timestamp: 1543789473\n",
      "  timesteps_since_restore: 74000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 413 s, 74 iter, 74000 ts, -15.9 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-39\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -15.954\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 75000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.01\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 592000\n",
      "    num_target_updates: 148\n",
      "    opt_peak_throughput: 4564.732\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.354\n",
      "    sample_time_ms: 11.273\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.9885\n",
      "  time_since_restore: 418.5251269340515\n",
      "  time_this_iter_s: 5.480872869491577\n",
      "  time_total_s: 418.5251269340515\n",
      "  timestamp: 1543789479\n",
      "  timesteps_since_restore: 75000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 418 s, 75 iter, 75000 ts, -16 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-12-02_14-24-44\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.5\n",
      "  episode_reward_mean: -16.0685\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 76000\n",
      "  experiment_id: e64e0f202d094d40a940a456ddf5bd06\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.284\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 600000\n",
      "    num_target_updates: 150\n",
      "    opt_peak_throughput: 4393.465\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.326\n",
      "    sample_time_ms: 11.923\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 10.142.38.66\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 4140\n",
      "  policy_reward_mean:\n",
      "    vehicles: -4.017125\n",
      "  time_since_restore: 423.9374988079071\n",
      "  time_this_iter_s: 5.412371873855591\n",
      "  time_total_s: 423.9374988079071\n",
      "  timestamp: 1543789484\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 9.8/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=4140], 423 s, 76 iter, 76000 ts, -16.1 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup policies for each vehicle\n",
    "\n",
    "network_name = 'Braess'\n",
    "nb_veh = 4\n",
    "# network init should build the Network object\n",
    "# ----- TO DO -----: import the network class\n",
    "network = Networks.network(network_name, nb_veh)\n",
    "# nb_path should be a property method\n",
    "\n",
    "\"\"\"\n",
    "define a function (class instantiation) which have for parameter a network name, \n",
    "and the number of vehicles, \n",
    "the return the num paths.\n",
    "\n",
    "interface of the class network\n",
    "class network:\n",
    "    def __init__(self, network_name, nb_veh):\n",
    "        load the network which correspond to the network_name\n",
    "        define the nb_veh as the nb_veh\n",
    "        from nb_veh and the intern demand define the number of flow that each veh represent\n",
    "        also define __nb_paths to give it to the Env\n",
    "\n",
    "    @property\n",
    "    def nb_paths(self):\n",
    "        return self.__nb_paths\n",
    "\"\"\"\n",
    "\n",
    "nb_path = network.nb_paths\n",
    "\n",
    "env_config = {\n",
    "    'network': network_name,\n",
    "    'num_veh': nb_veh,\n",
    "    'num_paths': nb_path,\n",
    "    'soc_fac': 0 # to change\n",
    "}\n",
    "routing_env = RoutingEnv(env_config)\n",
    "car_obs_space = routing_env.observation_space\n",
    "car_act_space = routing_env.action_space\n",
    "config = {\"gamma\": 0.0}\n",
    "policy_graphs = {\n",
    "    'vehicles': (DQNPolicyGraph, car_obs_space, car_act_space, config)\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Begin\")\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: RoutingEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'DQN',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 100\n",
    "            },\n",
    "            'config': {\n",
    "                'env_config': env_config,\n",
    "                'multiagent': {\n",
    "                    'policy_graphs': policy_graphs,\n",
    "                    'policy_mapping_fn': tune.function(lambda agent_id: 'vehicles')\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    print(\"End\")\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tt_mc(action_dict, network, soc_fact):\n",
    "    paths_flow_dict = {}\n",
    "    for agent, rl_action in action_dict.items():\n",
    "        rl_action = int(rl_action)\n",
    "        if rl_action in paths_flow_dict:\n",
    "            paths_flow_dict[rl_action] += 1\n",
    "        else:\n",
    "            paths_flow_dict[rl_action] = 1\n",
    "    network.update_flow_from_dict(paths_flow_dict)\n",
    "\n",
    "    travel_time = {}\n",
    "    marginal_cost = {}\n",
    "    for agent, path_choice in action_dict.items():\n",
    "        path_choice = int(path_choice)\n",
    "        travel_time[agent] = network.travel_time(path_choice)\n",
    "        marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "    rew_dict = reward_calculator(travel_time, marginal_cost, soc_fact)\n",
    "    return travel_time, marginal_cost, rew_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "file = open(\"/Users/theophile/Documents/Classes/FLOW/Project/learning_wardrop/test_lambda_1_gamma_0_trail_1\", 'r')\n",
    "j = 0\n",
    "# we want to plot the evolution of the path choice, of the reward and of the travel time\n",
    "Actions_plot = np.array([[0, 0, 0, 0]])\n",
    "Reward_plot = np.array([[0, 0, 0, 0]])\n",
    "Travel_time_plot = np.array([[0, 0, 0, 0]])\n",
    "while(True):\n",
    "    j = j+1\n",
    "    try:\n",
    "        actions = file.readline()\n",
    "        rewards = file.readline()\n",
    "        action_dict = ast.literal_eval(\"{\" + actions.split('{')[1].split('}')[0]+ \"}\")\n",
    "        reward_dict = ast.literal_eval(\"{\" + rewards.split('{')[1].split('}')[0]+ \"}\")\n",
    "    \n",
    "        network = Networks.network(network_name, nb_veh)\n",
    "        travel_time, marginal_cost, rew_dict = get_tt_mc(action_dict, network, 1)\n",
    "        \n",
    "        actions_np = np.fromiter(action_dict.values(), dtype=int)\n",
    "        Actions_plot = np.append(Actions_plot, [actions_np], axis=0)\n",
    "        rewards_np = np.fromiter(reward_dict.values(), dtype=float)\n",
    "        Reward_plot = np.append(Reward_plot, [rewards_np], axis=0)\n",
    "        travel_time_np = np.fromiter(travel_time.values(), dtype=float)\n",
    "        Travel_time_plot = np.append(Travel_time_plot, [travel_time_np], axis=0)\n",
    "        if(j==1):\n",
    "            print(\"------ First iteration ------\")\n",
    "            print(\"Path choice: \" + str(action_dict))\n",
    "            print(\"Reward ray: \" + str(reward_dict))\n",
    "            print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "            print(\"Travel time cars: \" + str(travel_time))\n",
    "            print(\"Marginal cost: \" + str(marginal_cost))\n",
    "            print(\"Reward network: \" + str(rew_dict))\n",
    "    except:\n",
    "        print()\n",
    "        print(\"------ Last iteration ------\")\n",
    "        print(\"Path choice: \" + str(action_dict))\n",
    "        print(\"Reward ray: \" + str(reward_dict))\n",
    "        print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "        print(\"Travel time cars: \" + str(travel_time))\n",
    "        print(\"Marginal cost: \" + str(marginal_cost))\n",
    "        print(\"Reward network: \" + str(rew_dict))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(Actions_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Path choice of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(Reward_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Rewards of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(Travel_time_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Travel time of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Environment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
