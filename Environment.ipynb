{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KS6wwFRc9CQK"
   },
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cslVfkBSLean"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import Networks\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_policy_graph import *\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.models.preprocessors import DictFlatteningPreprocessor, Preprocessor\n",
    "\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8YRoyVY687z"
   },
   "source": [
    "### Environment Below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73fz3Ogq6870"
   },
   "outputs": [],
   "source": [
    "def reward_calculator(travel_time, marginal_cost, soc_fac):\n",
    "    rew_dict = {}\n",
    "    for agent in travel_time.keys():\n",
    "        rew_dict[agent] = - (travel_time[agent] + soc_fac * marginal_cost[agent])\n",
    "    return rew_dict\n",
    "\n",
    "## Routing Environment \n",
    "class RoutingEnv(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        The cars start at the same origin point, Point A, and need to reach the same destination, Point Z. \n",
    "        Each car can reach Point Z via a variety of routing choices described in a given network, in which \n",
    "        each route introduces different travel times and congestion. \n",
    "        The goal is to minimize the average travel times amongst each of the cars.\n",
    "    \n",
    "    Observation: \n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tObservation                 Min          Max\n",
    "        0\tPrevious Route Choice        0      total_routes-1\n",
    "        1\tRoute Travel Time            0           +Inf\n",
    "        2\tComm Message               -Inf          +Inf\n",
    "        \n",
    "    Actions:\n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tAction                      Min          Max\n",
    "        0\tFuture Path_Choice           0      total_routes-1\n",
    "        1\tComm Message               -Inf          +Inf\n",
    "            \n",
    "    Reward:\n",
    "        Reward for each car is determined by the following formula: \n",
    "        marginal_cost = d[t(x_e)]/d[x_e]\n",
    "        Cost = route_travel_time + λ(marginal_cost)\n",
    "        Reward = -Cost\n",
    "        ***\n",
    "        route_travel_time: Travel time of the route previously taken by the car\n",
    "        marginal_cost: Cost that the car's route choice imposes on everyone else. \n",
    "                       The formula above captures the change in the travel flow \n",
    "                       with respect to the change in vehicle flow on a given road.\n",
    "        λ: Weight Toward Social Good (between 0 and 1)\n",
    "        \n",
    "    \n",
    "    Starting State:\n",
    "        All observations are assigned -1 for path choice and travel times.\n",
    "    \n",
    "    Episode Termination:\n",
    "        Cars keeps a consistent routing distribution.\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average travel time is less than or equal to the theorical social optimum. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        self.network_name = config['network']\n",
    "        self.num_paths = config['num_paths']\n",
    "        self.soc_fac = config['soc_fac']\n",
    "        self.num_veh = config['num_veh']\n",
    "        self.num_obs = 2\n",
    "        self.num_actions = 1\n",
    "        self.state = None\n",
    "        # Make observation space\n",
    "        obs_spaces = {\n",
    "            'prev_route': Discrete(self.num_paths),\n",
    "            'prev_time': Box(low=0, \n",
    "                             high=float('+inf'), \n",
    "                             shape=(1,), \n",
    "                             dtype=np.float32)\n",
    "        }\n",
    "        self.preprocessor = DictFlatteningPreprocessor(Dict(obs_spaces))\n",
    "        self.observation_space = self.preprocessor.observation_space\n",
    "        # Make the action space\n",
    "        self.action_space = Discrete(self.num_paths) # int between 0 and num_paths-1\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        # Create initial observations for each vehicle\n",
    "        start = {\n",
    "            'prev_route': 0,\n",
    "            'prev_time': 0\n",
    "        }\n",
    "        self.state = {'car_{}'.format(i): self.preprocessor.transform(start) for i in range(self.num_veh)}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        # Apply the actions of every agent at the same time\n",
    "        paths_flow_dict = {}\n",
    "        for agent, rl_action in action_dict.items():\n",
    "            # agent is one string that represent the id of the agent\n",
    "            # rl_action is one number that represent the path choice of the agent,\n",
    "            # rl_action should be a int between 0 and nb_paths-1\n",
    "            rl_action = int(rl_action)\n",
    "            assert type(rl_action) == int and rl_action > -1 and rl_action < network.nb_paths\n",
    "            # we built a dictionnary paths_flow_dict that store the path flow on every path\n",
    "            if rl_action in paths_flow_dict:\n",
    "                paths_flow_dict[rl_action] += 1\n",
    "            else:\n",
    "                paths_flow_dict[rl_action] = 1\n",
    "\n",
    "        # update the path travel times of the network given the path flows\n",
    "        network.update_flow_from_dict(paths_flow_dict)\n",
    "        \n",
    "        \n",
    "        # Calculate states, reward, and done for each agent\n",
    "        travel_time = {}\n",
    "        marginal_cost = {}\n",
    "        \n",
    "        for agent, path_choice in action_dict.items():\n",
    "            path_choice = int(path_choice)\n",
    "            assert type(path_choice) == int and path_choice > -1 and path_choice < network.nb_paths\n",
    "            # network travel time ( path ) return the travel time of the path\n",
    "            travel_time[agent] = network.travel_time(path_choice)\n",
    "            # network marginal cost ( path ) return the marginal cost of the path\n",
    "            marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "            new_obs = {\n",
    "                'prev_route': path_choice,\n",
    "                'prev_time': travel_time[agent]\n",
    "            }\n",
    "            obs_dict[agent] = self.preprocessor.transform(new_obs)\n",
    "            # Cost is the path_time\n",
    "            # rew_dict[agent] = reward_calculator(agent, marginal_cost)\n",
    "            # -path_choice # TO-DO: CHANGE THIS! \n",
    "            # Set done and infos\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "        rew_dict = reward_calculator(travel_time, marginal_cost, self.soc_fac)\n",
    "        self.state = obs_dict   \n",
    "        done[\"__all__\"] = True\n",
    "         \n",
    "        return obs_dict, rew_dict, done, info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code runs the experiment for the multiagent problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2018-11-30_13-38-18_99568/logs.\n",
      "Waiting for redis server at 127.0.0.1:54926 to respond...\n",
      "Waiting for redis server at 127.0.0.1:27127 to respond...\n",
      "Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=878469f6e608a5d58f8c510c169e0d60594d1bb77fe69d60\n",
      "======================================================================\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.0/17.2 GB\n",
      "\n",
      "Created LogSyncer for /Users/theophile/ray_results/route-DQN/DQN_multi_routing_0_2018-11-30_13-38-19dsyf9uc3 -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.0/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_13-38-31\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -14.9865\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 1000\n",
      "  experiment_id: bb2e2eb61b2249569018f2a61b156373\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 1\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 8.834\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 99582\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.746625\n",
      "  time_since_restore: 2.3592119216918945\n",
      "  time_this_iter_s: 2.3592119216918945\n",
      "  time_total_s: 2.3592119216918945\n",
      "  timestamp: 1543613911\n",
      "  timesteps_since_restore: 1000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=99582], 2 s, 1 iter, 1000 ts, -15 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_13-38-36\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.004\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 2000\n",
      "  experiment_id: bb2e2eb61b2249569018f2a61b156373\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.759\n",
      "    max_exploration: 0.902\n",
      "    min_exploration: 0.902\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8000\n",
      "    num_target_updates: 3\n",
      "    opt_peak_throughput: 4734.261\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.467\n",
      "    sample_time_ms: 9.478\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 99582\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.751\n",
      "  time_since_restore: 7.607595920562744\n",
      "  time_this_iter_s: 5.24838399887085\n",
      "  time_total_s: 7.607595920562744\n",
      "  timestamp: 1543613916\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=99582], 7 s, 2 iter, 2000 ts, -15 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_13-38-45\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.073\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 4000\n",
      "  experiment_id: bb2e2eb61b2249569018f2a61b156373\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.517\n",
      "    max_exploration: 0.706\n",
      "    min_exploration: 0.706\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 24000\n",
      "    num_target_updates: 7\n",
      "    opt_peak_throughput: 4909.998\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.773\n",
      "    sample_time_ms: 9.328\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 99582\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.76825\n",
      "  time_since_restore: 16.947008848190308\n",
      "  time_this_iter_s: 4.712668180465698\n",
      "  time_total_s: 16.947008848190308\n",
      "  timestamp: 1543613925\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.6/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=99582], 16 s, 4 iter, 4000 ts, -15.1 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_13-38-55\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.268\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 6000\n",
      "  experiment_id: bb2e2eb61b2249569018f2a61b156373\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.003\n",
      "    max_exploration: 0.51\n",
      "    min_exploration: 0.51\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 40000\n",
      "    num_target_updates: 11\n",
      "    opt_peak_throughput: 4569.441\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.214\n",
      "    sample_time_ms: 10.365\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 99582\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.817\n",
      "  time_since_restore: 26.506409645080566\n",
      "  time_this_iter_s: 4.885345935821533\n",
      "  time_total_s: 26.506409645080566\n",
      "  timestamp: 1543613935\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=99582], 26 s, 6 iter, 6000 ts, -15.3 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_13-39-00\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.3735\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 7000\n",
      "  experiment_id: bb2e2eb61b2249569018f2a61b156373\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.767\n",
      "    max_exploration: 0.41200000000000003\n",
      "    min_exploration: 0.41200000000000003\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 48000\n",
      "    num_target_updates: 13\n",
      "    opt_peak_throughput: 4728.64\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.964\n",
      "    sample_time_ms: 9.882\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 99582\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.843375\n",
      "  time_since_restore: 31.653084754943848\n",
      "  time_this_iter_s: 5.146675109863281\n",
      "  time_total_s: 31.653084754943848\n",
      "  timestamp: 1543613940\n",
      "  timesteps_since_restore: 7000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=99582], 31 s, 7 iter, 7000 ts, -15.4 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_13-39-05\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.5415\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 8000\n",
      "  experiment_id: bb2e2eb61b2249569018f2a61b156373\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.194\n",
      "    max_exploration: 0.31400000000000006\n",
      "    min_exploration: 0.31400000000000006\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 56000\n",
      "    num_target_updates: 15\n",
      "    opt_peak_throughput: 4448.199\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.273\n",
      "    sample_time_ms: 10.864\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 99582\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.885375\n",
      "  time_since_restore: 36.83364486694336\n",
      "  time_this_iter_s: 5.180560111999512\n",
      "  time_total_s: 36.83364486694336\n",
      "  timestamp: 1543613945\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=99582], 36 s, 8 iter, 8000 ts, -15.5 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_13-39-10\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.6265\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 9000\n",
      "  experiment_id: bb2e2eb61b2249569018f2a61b156373\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 6.471\n",
      "    max_exploration: 0.21599999999999997\n",
      "    min_exploration: 0.21599999999999997\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 64000\n",
      "    num_target_updates: 17\n",
      "    opt_peak_throughput: 4945.275\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.739\n",
      "    sample_time_ms: 9.256\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 99582\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.906625\n",
      "  time_since_restore: 41.88171887397766\n",
      "  time_this_iter_s: 5.048074007034302\n",
      "  time_total_s: 41.88171887397766\n",
      "  timestamp: 1543613950\n",
      "  timesteps_since_restore: 9000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - DQN_multi_routing_0:\tRUNNING [pid=99582], 41 s, 9 iter, 9000 ts, -15.6 rew\n",
      "\n",
      "Result for DQN_multi_routing_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2018-11-30_13-39-15\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: -14.0\n",
      "  episode_reward_mean: -15.888\n",
      "  episode_reward_min: -17.0\n",
      "  episodes_this_iter: 1000\n",
      "  episodes_total: 10000\n",
      "  experiment_id: bb2e2eb61b2249569018f2a61b156373\n",
      "  hostname: C02X23AUJHD3\n",
      "  info:\n",
      "    grad_time_ms: 7.297\n",
      "    max_exploration: 0.118\n",
      "    min_exploration: 0.118\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 72000\n",
      "    num_target_updates: 19\n",
      "    opt_peak_throughput: 4385.512\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.276\n",
      "    sample_time_ms: 10.85\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 128.32.46.247\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 99582\n",
      "  policy_reward_mean:\n",
      "    vehicles: -3.972\n",
      "  time_since_restore: 46.93601369857788\n",
      "  time_this_iter_s: 5.05429482460022\n",
      "  time_total_s: 46.93601369857788\n",
      "  timestamp: 1543613955\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "TERMINATED trials:\n",
      " - DQN_multi_routing_0:\tTERMINATED [pid=99582], 46 s, 10 iter, 10000 ts, -15.9 rew\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 10.7/17.2 GB\n",
      "Result logdir: /Users/theophile/ray_results/route-DQN\n",
      "TERMINATED trials:\n",
      " - DQN_multi_routing_0:\tTERMINATED [pid=99582], 46 s, 10 iter, 10000 ts, -15.9 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup policies for each vehicle\n",
    "\n",
    "network_name = 'Braess'\n",
    "nb_veh = 4\n",
    "# network init should build the Network object\n",
    "# ----- TO DO -----: import the network class\n",
    "network = Networks.network(network_name, nb_veh)\n",
    "# nb_path should be a property method\n",
    "\n",
    "\"\"\"\n",
    "define a function (class instantiation) which have for parameter a network name, \n",
    "and the number of vehicles, \n",
    "the return the num paths.\n",
    "\n",
    "interface of the class network\n",
    "class network:\n",
    "    def __init__(self, network_name, nb_veh):\n",
    "        load the network which correspond to the network_name\n",
    "        define the nb_veh as the nb_veh\n",
    "        from nb_veh and the intern demand define the number of flow that each veh represent\n",
    "        also define __nb_paths to give it to the Env\n",
    "\n",
    "    @property\n",
    "    def nb_paths(self):\n",
    "        return self.__nb_paths\n",
    "\"\"\"\n",
    "\n",
    "nb_path = network.nb_paths\n",
    "\n",
    "env_config = {\n",
    "    'network': network_name,\n",
    "    'num_veh': nb_veh,\n",
    "    'num_paths': nb_path,\n",
    "    'soc_fac': 0.0 # to change\n",
    "}\n",
    "routing_env = RoutingEnv(env_config)\n",
    "car_obs_space = routing_env.observation_space\n",
    "car_act_space = routing_env.action_space\n",
    "config = {\"gamma\": 0.85}\n",
    "policy_graphs = {\n",
    "    'vehicles': (DQNPolicyGraph, car_obs_space, car_act_space, config)\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: RoutingEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'DQN',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 10\n",
    "            },\n",
    "            'config': {\n",
    "                'env_config': env_config,\n",
    "                'multiagent': {\n",
    "                    'policy_graphs': policy_graphs,\n",
    "                    'policy_mapping_fn': tune.function(lambda agent_id: 'vehicles')\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Environment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
