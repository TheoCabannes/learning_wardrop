{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Environment.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/TheoCabannes/learning_wardrop/blob/master/Environment.ipynb","timestamp":1540600328298}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"KS6wwFRc9CQK","colab_type":"text"},"cell_type":"markdown","source":["# Building the environment"]},{"metadata":{"id":"cslVfkBSLean","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install gym\n","!pip install ray[rllib]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g1rfbb6b687S","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MEFH9bAd88O_","colab_type":"text"},"cell_type":"markdown","source":["The class link is defined "]},{"metadata":{"id":"WQAi1vqp687U","colab_type":"code","colab":{}},"cell_type":"code","source":["class link:\n","    \"\"\"\n","    \"\"\"\n","    def __init__(self, name, a0, a1):\n","        self.__name = name\n","        # self.__travel_time = 0 # the travel time of the link\n","        # self.__flow = 0 # the flow on the link\n","        self.__paths = [] # the paths using this link\n","        self.__a0 = a0\n","        self.__a1 = a1\n","        \n","    def add_path(self, path):\n","        self.__paths.append(path)\n","        \n","    def get_travel_time(self):\n","        return self.__travel_time\n","    \n","    def update_flow(self):\n","        self.__flow = 0\n","        for path in self.__paths:\n","            self.__flow += path.get_flow()\n","    \n","    def update_travel_time(self):\n","        self.update_flow()\n","        self.__travel_time = self.__a0 + self.__a1 * self.__flow\n","        \n","    def __str__(self):\n","        return self.__name"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_rzDB2yv8_xM","colab_type":"text"},"cell_type":"markdown","source":["The class path is ..."]},{"metadata":{"id":"QGgF8UUS687W","colab_type":"code","colab":{}},"cell_type":"code","source":["class path:\n","    \"\"\"\n","    Notes: \n","     - We should add a method that \"adds a car/updates the flow\" to a path. \n","       This should include the update_travel_time method.\n","     - The travel time of each path shouldn't start at 0, since each path always has some base travel time.\n","       Have the initial travel times of the path be when the congestion is 0. \n","     - When you \"update the travel time\", add what the travel time of each road would be given\n","       the path's current flow.\n","       Maybe each link in the path should only have a function that calculates it's travel time\n","       given a flow on that link.\n","    \"\"\"\n","    def __init__(self, name, links):\n","        self.__name = name\n","        # self.__travel_time = 0 # the travel time of the paths\n","        self.__flow = 0 # the flow on the paths\n","        self.__links = links # the paths using this link\n","        for link in links:\n","            link.add_path(self)\n","    \n","    def get_flow(self):\n","        return self.__flow\n","    \n","    def update_flow(self, f):\n","        self.__flow = f\n","    \n","    def add_flow(self, f):\n","        self.__flow += f\n","      \n","    # add the marginal cost to see if you converge to the social optimum\n","    def get_travel_time(self):\n","        return self.__travel_time\n","    \n","    def update_travel_time(self):\n","        self.__travel_time = 0\n","        for link in self.__links:\n","            self.__travel_time += link.get_travel_time()\n","            \n","    def __str__(self):\n","        return self.__name"],"execution_count":0,"outputs":[]},{"metadata":{"id":"79V69uV1-63-","colab_type":"text"},"cell_type":"markdown","source":["The class network ..."]},{"metadata":{"id":"juwAXyXo687Z","colab_type":"code","colab":{}},"cell_type":"code","source":["class network:\n","  # add a method that gives the travel time\n","    def __init__(self, paths, links):\n","        self.paths = paths\n","        self.links = links\n","    \n","    def update(self):\n","        for link in self.links:\n","            link.update_travel_time()\n","        for path in self.paths:\n","            path.update_travel_time()\n","            \n","    def update_av(self, avs):\n","        for path in self.paths:\n","            path.update_flow(0)\n","        for av in avs:\n","            av.path.add_flow(av.flow)\n","        self.update()\n","        for av in avs:\n","            av.give_reward()\n","            \n","    def reset(self):\n","      for path in self.paths:\n","        path.update_flow(0)\n","      self.update()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l0qPM_kN687b","colab_type":"code","colab":{}},"cell_type":"code","source":["ab = link('ab', 1, 1/100)\n","ac = link('ac', 2, 0)\n","bc = link('bc', 0.25, 0)\n","bd = link('bd', 2, 0)\n","cd = link('cd', 1, 1/100)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qdo_LozV687c","colab_type":"code","colab":{}},"cell_type":"code","source":["abcd = path('abcd', [ab, bc, cd])\n","abd = path('abd', [ab, bd])\n","acd = path('acd', [ac, cd])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-Is0J4d1687e","colab_type":"code","colab":{}},"cell_type":"code","source":["braess = network([abd, acd, abcd], [ab, ac, bc, bd, cd])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QEhpomC1687g","colab_type":"code","colab":{}},"cell_type":"code","source":["braess.update()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"khKBey_6687j","colab_type":"code","colab":{}},"cell_type":"code","source":["#Nash equilibrium\n","abcd.update_flow(50)\n","abd.update_flow(25)\n","acd.update_flow(25)\n","braess.update()\n","print([path.get_travel_time() for path in braess.paths])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TnjO9F3Z687m","colab_type":"code","colab":{}},"cell_type":"code","source":["#social optimum\n","abcd.update_flow(0)\n","abd.update_flow(50)\n","acd.update_flow(50)\n","braess.update()\n","print([path.get_travel_time() for path in braess.paths])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_8I_I4sS687o","colab_type":"code","colab":{}},"cell_type":"code","source":["## change this\n","# 1. to do Igor experiment \n","class autonomous_vehicle:\n","    # p = 0.5\n","    def __init__(self, name, flow):\n","        self.name = name\n","        self.path = None\n","        self.reward = 0\n","        self.flow = flow\n","    \n","    def path_choice(self, path):\n","        if self.path == None:\n","            self.path = path\n","        # the following condition makes the system converges toward Nash\n","        # change this condition to make an faster convergence toward Nash\n","        p = abs(self.path.get_travel_time() - path.get_travel_time()) / self.path.get_travel_time()\n","        self.path = (path if np.random.rand() < p else self.path)\n","    \n","    def give_reward(self):\n","        # add here a NN which choose the path given the reward\n","        self.reward = - self.path.get_travel_time()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-db59fTa687p","colab_type":"code","colab":{}},"cell_type":"code","source":["av1 = autonomous_vehicle('1', 25)\n","av2 = autonomous_vehicle('2', 25)\n","av3 = autonomous_vehicle('3', 25)\n","av4 = autonomous_vehicle('4', 25)\n","\n","avs = [av1, av2, av3, av4]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uhdyz6Eg687r","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","braess.reset()\n","\n","for i in range(100):\n","    best_path = [x for _,x in sorted([(path.get_travel_time(),path) for path in braess.paths],  key=lambda tup: tup[0])][0]\n","    #print([(str(path), path.flow, path.get_travel_time()) for path in braess.paths])\n","    #print(av2.p)\n","    for av in avs:\n","        av.path_choice(best_path)\n","\n","    braess.update_av(avs)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"vakJuSxh687t","colab_type":"code","colab":{}},"cell_type":"code","source":["print([str(av.path) for av in avs])\n","print([(str(path), path.get_flow(), path.get_travel_time()) for path in braess.paths])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OnH1qTtAXP2A","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create the roads\n","        ab = link('ab', 1, 1/100)\n","        ac = link('ac', 2, 0)\n","        bc = link('bc', 0.25, 0)\n","        bd = link('bd', 2, 0)\n","        cd = link('cd', 1, 1/100)\n","        roads = [ab, ac, bc, bd, cd]\n","        \n","        # Create the possible routes\n","        abcd = path('abcd', [ab, bc, cd])\n","        abd = path('abd', [ab, bd])\n","        acd = path('acd', [ac, cd])\n","        paths = [abd, acd, abcd]\n","        \n","        # Put the roads and the routes in a network\n","        town = network(paths, links)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k8YRoyVY687z","colab_type":"text"},"cell_type":"markdown","source":["### Environment Description\n","The environment\n"]},{"metadata":{"id":"xsTdNOPa-N7h","colab_type":"code","colab":{}},"cell_type":"code","source":["import ray\n","from gym.spaces import *\n","from ray.rllib.env import *"],"execution_count":0,"outputs":[]},{"metadata":{"id":"73fz3Ogq6870","colab_type":"code","colab":{}},"cell_type":"code","source":["## Routing Environment \n","class RoutingEnv(MultiAgentEnv):\n","    \"\"\"A partially observed routing environment in which cars are choosing \n","       routing paths each day.\n","    \n","    States\n","        Each vehicle knows: \n","         - It's own previous path taken\n","         - Previous path's travel time\n","         - Communication from the other vehicles\n","    Actions\n","        Each vehicle will\n","         - Choose it's next path to take\n","         - Choose to send some communication symbol\n","    Rewards\n","        Each vehicle's goal is to minimize their own travel time and eventually\n","        reduce their marginal cost of their choices on each driver. \n","        The cost function is: \n","            (1) the amount of time on your own route \n","            (2) the cost of your route choice on the other drivers \n","                    (how much the travel times of other drivers has increased.\n","    Termination\n","        A rollout is terminated if the time horizon is reached.\n","        \n","    *****Note: Right now, there is no communication implemented here. \n","         CHANGE LATER\n","    \"\"\"\n","    \n","    def __init__(self, config):\n","        \"\"\"Creates the routing environment for the RL training.\n","        \n","        config: A dict with following keys\n","           - network_name: Name of the road scenario.\n","        \"\"\"\n","        # TO-DO: IMPLEMENT CREATE_NETWORK\n","        self.__network = create_network(config[\"network_name\"]) \n","        self.__n_actions = # Total number of possible actions\n","        self.__n_states = 3\n","        self.__n_vehicles = # Total number of vehicles or flows in this example.\n","        self.__reward_calculator = \n","        self.__T = config[\"time_horizon\"]\n","      \n","    \n","    @property\n","    def action_space(self):\n","        \"\"\" Defines the structure of the actions this env will give.\n","        \n","        Returns: A Discrete space of size n, where n is the number of actions.\n","        \n","        A Discrete space only allow the vehicles to output values within a \n","        fixed range. The range of values will depend upon the number of paths \n","        in the network. \n","        \n","        Note: Depending on what we want the vehicles to communicate, this may \n","              change. Also, do we account for the number of agents here?\n","        \"\"\"\n","        return Discrete(self.__n_actions) \n","\n","    @property\n","    def observation_space(self):\n","        \"\"\" Defines the structure of the observations this env will give.\n","       \n","        Returns: A box with shape m x n, where:\n","          m: number of agents\n","          n: number of observations per agent\n","        \"\"\"\n","        # Question: Not sure if the shape values are in the right order.\n","        return Box(shape=(self.__n_vehicles, self.__n_states), dtype=np.float32)\n","      \n","    def get_state(self, **kwargs):\n","        ####################################\n","        # specify desired state space here #\n","        ####################################\n","        return  ### FILL IN ###\n","    \n","    def reset(self):\n","        \"\"\"Restarts the environment.\n","        \n","        Returns: A dictionary, representing the initial state\n","          - Key: Vehicle \n","          - Value: Dictionary \n","                 *Key: \"Path\", \"Time\", \"Message\"\n","                 *Value: Corresponding values\n","        \"\"\"\n","        route_flows = network.reset()\n","        obs_dict = {}\n","        for veh in range(self.__n_vehicles):\n","          obs_dict[veh] = {\"path\": None, \"time\": -1, \"message\": None}\n","        # Set starting state\n","        self.__curr_state = obs_dict\n","        # Start count for env\n","        self__count = 0\n","        return obs_dict\n","    \n","    def step(self, actions):\n","        \"\"\"Moves the environment forward one step by applying the actions of \n","           the agents.\n","           \n","        What is the transition function in this step? \n","         - NOT SURE! \n","           \n","        Returns: \n","          next_state_obs_dict: A dictionary representing the next state in the \n","                               environment\n","          reward_dict: A dictionary of the rewards of actions taken by every \n","                       vehicle\n","          done: A boolean representing if the simulation is finished\n","          info_dict: A dictionary of extra information? \n","          \n","        \"\"\"\n","        next_state = {}\n","        reward_dict = {}\n","        done = False\n","        info_dict = {}\n","        \n","        # Apply the actions of every agent at the same time\n","        for agent, rl_action in action.items():\n","            # TO-DO: IMPLEMENT ADDTOPATH\n","            network.addToPath(agent, rl_action) # Assumes no messages\n","            next_state[agent] = {\"path\": rl_action, \n","                                 \"time\": None, \n","                                 \"message\": None}\n","            \n","        # Calculate the path times and compute reward\n","        for agent, rl_action in action.items():\n","            # TO-DO: IMPLEMENT PATHTIME\n","            path_time = network.pathTime(rl_action)\n","            next_state[agent][\"time\"] = path_time\n","            # TO-DO: IMPLEMENT COMPUTE REWARD\n","            reward_dict[agent] = self.computer_reward(self.__curr_state[agent], \n","                                                      rl_action)\n","        # Determine if the environment is finished\n","        if self.__count >= self.__T:\n","          done = True\n","        self.__count++\n","         \n","        return next_state, reward_dict, done, info_dict\n","        \n","    def compute_reward(self, state, rl_actions, **kwargs):\n","        \"\"\"Computes the reward of each action for the current state.\"\"\"\n","        reward = 0\n","        return reward\n","\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"WyXOaj4ySreU","colab_type":"code","colab":{}},"cell_type":"code","source":["# add the gym running session\n","# give space for the neural network "],"execution_count":0,"outputs":[]},{"metadata":{"id":"5kAopatw8chi","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def create_model(num_features,num_outputs):\n","    \n","    \n","    # create the placeholders\n","    input_ph = tf.placeholder(tf.float32, [None, num_features])\n","    output_ph = tf.placeholder(tf.float32, [None, num_outputs])\n","\n","    # create a fully connected network layer\n","    hidden_layer1 = tf.contrib.layers.fully_connected(\n","        input_ph,                                               # input placeholder\n","        32,                                                     # number of hidden nodes\n","        weights_initializer=tf.truncated_normal_initializer(),  # initializer for the weights\n","        activation_fn=tf.nn.relu,                               # nonlinearity (None means linear combination)\n","        scope=\"hidden1\",                                         # name of the node\n","        )\n","    # create a fully connected network layer\n","    hidden_layer2 = tf.contrib.layers.fully_connected(\n","        hidden_layer1,                                               # input placeholder\n","        32,                                                     # number of hidden nodes\n","        weights_initializer=tf.truncated_normal_initializer(),  # initializer for the weights\n","        activation_fn=tf.nn.relu,                               # nonlinearity (None means linear combination)\n","        scope=\"hidden2\",                                         # name of the node\n","        )\n","\n","    # create an output layer\n","    output_layer = tf.contrib.layers.fully_connected(\n","        hidden_layer2,                                           # input placeholder\n","        num_outputs,                                            # number of hidden nodes\n","        weights_initializer=tf.truncated_normal_initializer(),  # initializer for the weights\n","        activation_fn=None,                                     # nonlinearity (None means linear combination)\n","        scope=\"output\",                                         # name of the node\n","        reuse=tf.AUTO_REUSE)\n","    \n","    return input_ph, output_ph, output_layer\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ozCghOF3Zkpf","colab_type":"code","colab":{}},"cell_type":"code","source":["# create a tensorflow session\n","sess = tf.Session()\n","\n","# create loss\n","mse = tf.reduce_mean(0.5 * tf.square(output_pred - output_ph))\n","\n","# create optimizer\n","opt = tf.train.AdamOptimizer().minimize(mse)\n","\n","# initialize variables\n","sess.run(tf.global_variables_initializer())\n","\n","# create saver to save model variables\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]}]}